<!DOCTYPE html>
<html>
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-L9WV4KKVGH"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-L9WV4KKVGH');
    </script>
    <title>planet davorg</title>
    <!-- Bootstrap core CSS -->
    <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet">

    <link rel="alternate" type="application/atom+xml" title="Atom" href="https://davorg.theplanetarium.org/" />
    <style type="text/css">
body {
  padding-top: 75px;
}
    </style>

  </head>
  <body>
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">planet davorg</a>
        </div>
        <!-- div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#">Home</a></li>
            <li><a href="#about">About</a></li>
            <li><a href="#contact">Contact</a></li>
          </ul>
        </div --><!--/.nav-collapse -->
      </div>
    </nav>
    <div class="container">
      <div class="row">
        <div class="col-md-2">
          <h2>sources</h2>
          <ul>
            <li><a href="https://blog.dave.org.uk/">davblog</a></li>
            <li><a href="https://perlhacks.com/">perl hacks</a></li>
            <li><a href="https://dev.to/davorg">dev.to</a></li>
            <li><a href="https://davorg.medium.com/">medium</a></li>
          </ul>
          <p><a href="/atom.xml"><img alt="Feed" src="feed.gif" /> Subscribe</a>
          <br />
          <a href="/opml.xml"><img alt="OPML" src="opml.png" /> OPML</a></p>
          <p>Powered by <a href="https://metacpan.org/release/Perlanet">Perlanet</a></p>
        </div><!-- col-md-2 -->
        <div class="col-md-8">
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://perlhacks.com/2023/04/building-planets-with-perlanet-and-github/">Building Planets with Perlanet and GitHub</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><blockquote>
<p class="quoteText">Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape-descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea.</p>
<p style="text-align: right;"><em>Douglas Adams &#8211; The Hitchhiker&#8217;s Guide to the Galaxy</em></p>
</blockquote>
<p>I don&#8217;t still wear a digital watch, but I do like other things that are almost as unhip. In particular, I pine for the time about twenty years ago when web feeds looked like they were about to take over the world. Everyone had their favourite feed reader (I still miss Google Reader) and pretty much any useful web site would produce one or more web feeds that you could subscribe to and follow through your feed reader. For a few years, it was almost unthinkable to produce a web site without publishing a feed which included the changes to the site&#8217;s content.</p>
<p>Then, at some point, that changed. It wasn&#8217;t that web feeds vanished overnight. They still exist for many sites. But they are no longer ubiquitous. You can&#8217;t guarantee they&#8217;ll exist for every site you&#8217;re interested in. I remember people saying that social media would replace them. I was never convinced by that argument but, interestingly, one of the first times I noticed them vanishing was when Twitter removed their web feed of a user&#8217;s posts. They wanted people to use their AP instead (so I wrote <a href="https://github.com/davorg/twitter-json2atom">twitter-json2atom</a> that turned their API&#8217;s JSON into an Atom feed &#8211; I suspect it no longer works). Honestly, I think the main reason for the fall in popularity of web feeds was that people wanted you to read their content on their web sites where the interesting content was surrounded by uninteresting adverts.</p>
<p>But, as I said, not all web feeds vanished. There are still plenty of them out there (often, I expect because the sites&#8217; owners don&#8217;t realise they&#8217;re there or don&#8217;t know how to turn them off). And that means the web feed-driven technologies of the early 2000s can still be useful.</p>
<p>One such piece of technology is the feed aggregator. I remember these being very popular. You would create a web site and configure it with a list of web feeds that you were interested in. The site would be driven by a piece of software that every few hours would poll the web feeds in the configuration and use the information it found to create a) a web page made up of information from the feeds and b) another feed that contained all of the information from the source feeds. The most popular software for building these sites was called Planet Planet and was written in Python (it seems to have vanished sometime in the last twenty years, otherwise I would link to it). When I wrote a Perl version, I called it (for reasons I now regret) <a href="https://metacpan.org/pod/Perlanet">Perlanet</a>.</p>
<p>I still use Perlanet to build planet sites. And they&#8217;re all listed at <a href="https://theplanetarium.org/">The Planetarium</a>. Recently, I&#8217;ve started hosting all my planets on <a href="https://pages.github.com/">GitHub Pages</a>, using <a href="https://github.com/features/actions">GitHub Actions</a> to rebuild the sites periodically. I thought that maybe other people might be old-skool like me and might want to build their own planets &#8211; so in the rest of this post I&#8217;ll explain how to do that, using <a href="https://perl.theplanetarium.org/">Planet Perl</a> as an example.</p>
<p>The first thing you&#8217;ll need is a GitHub account and a repo to store the code for your planet. I&#8217;m going to assume you know how to set those up (in the interest of keeping this tutorial short). You only actually need two files to create a planet &#8211; a config file and a template for the web site.</p>
<p>Here&#8217;s part of the config for Planet Perl:</p><pre class="urvanov-syntax-highlighter-plain-tag">title: Planet Perl
description: There's More Than One Way To Aggregate It
url: https://perl.theplanetarium.org/
author:
  name: Dave Cross
  email: dave@theplanetarium.org
  twitter: davorg
entries: 75
entries_per_feed: 5
opml_file: docs/opml.xml
page:
  file: docs/index.html
  template: index.tt
feed:
  file: docs/atom.xml
  format: Atom
google_ga: G-HD966GMRYP
cutoff_duration:
  months: 1
feeds:
  - feed: https://www.perl.com/article/index.xml
    title: perl.com
    web: https://perl.com/
  - feed: https://news.perlfoundation.org/atom.xml
    title: Perl Foundation News
    web: https://news.perlfoundation.org/</pre><p>I&#8217;ve tried to make it self-explanatory. At the top, there are various config options for the output (the web page and the aggregated feed) and, below, are details of the feeds that you want to aggregate. Let&#8217;s look at the output options first.</p>
<ul>
<li>title and description: these are both strings that you can include on the web page that is created. They&#8217;re also used in the aggregate feed that is produced</li>
<li>url: this is where the web page will be available on the web</li>
<li>author: this contains details of the person publishing the aggregated site and feed. The Twitter handle is optional</li>
<li>entries: is the maximum number of entries that your output will contain in total</li>
<li>entries_per_feed: is the maximum number of entries that you will use from each of your feeds. This is to stop your output being swamped with entries from a particularly busy feed. This can be omitted, in which case there will be no limit</li>
<li>opml_file: OPML stands for &#8220;Outline Processor Markup File&#8221;. It used to be trendy to publish an OPML file which is a machine-readable data file which contains a list of the feeds that you are aggregating. These days, no-one cares. If you omit this setting, the file won&#8217;t be created.</li>
<li>page: this contains details of the web page you create. The template is the name of a template file that is used to create the HTML page (more on that below) and file is where the output page is written. If you keep the value used in my example, then things will work well with GitHub Pages as we&#8217;ll see later</li>
<li>feed: this contains details of the aggregate feed we create. You can choose a format (Atom or RSS) and the filename. Again, the default filename will work well with GitHub Pages</li>
<li>google_ga: if this value exists, then it will be used as the Google Analytics identifier for the web page that is created</li>
<li>cutoff_duration: this is another way to control which entries are used in your output feed. Any entries that were published longer ago than this period of time will be ignored</li>
</ul>
<p>Then we have the section of the config file that defines the feeds that we are going to aggregate. Each feed has three data items:</p>
<ul>
<li>feed: the URL of the feed</li>
<li>title: a string to use to describe the feed</li>
<li>web: the URL of the feed&#8217;s original web page</li>
</ul>
<p>And that&#8217;s all you need for the config file. Create that, put it in a file called &#8220;perlanetrc&#8221; and add it to your repo.</p>
<p>The other file you need is the template for the HTML page. This is usually called &#8220;index.tt&#8221;. <a href="https://github.com/davorg/planetperl/blob/master/index.tt">The one I use for Planet Perl</a> is rather complicated (there are all sorts of Javascript tricks in it). <a href="https://github.com/davorg/planetdavorg/blob/main/index.tt">The one I use for Planet Davorg</a> is far simpler &#8211; and should work well with the config file above. I suggest going with that initially and editing it once you&#8217;ve got everything else working.</p>
<p>I said those are the only two files you need. And that&#8217;s true. But the site you create will be rather ugly. My default web page uses <a href="https://getbootstrap.com/">Bootstrap</a> for CSS, but you&#8217;ll probably want to add your own CSS to tweak the way it looks &#8211; along with, perhaps, some Javascript and some images. All of the files that you need to make your site work should be added to the /docs directory in your repo.</p>
<p>Having got to this stage, we can test your web site. Well, we&#8217;ll need to install Perlanet first. There are two ways to do this. You can either install it from CPAN along with all of its (many) dependencies &#8211; using &#8220;cpan Perlanet&#8221; or there&#8217;s <a href="https://hub.docker.com/r/davorg/perl-perlanet">a Docker image</a> that you can use. Either way, once you have the software installed, running it is as simple as running &#8220;perlanet&#8221;. That will trundle along for a while and, when it has finished, you&#8217;ll find new files called &#8220;index.html&#8221; and &#8220;atom.xml&#8221; in the /docs directory. My favourite way to test the output locally is to use <a href="https://metacpan.org/dist/App-HTTPThis">App::HTTPThis</a>. Having installed this program, you can just run &#8220;http_this docs&#8221; from the repo&#8217;s main directory and then visit http://localhost:7007/index.html to see the site that was produced (or http://localhost:7007/atom.xml to see the feed.</p>
<p>You now have a system to build your new planet. You could run that on a server that&#8217;s connected to the internet and set up a cronjob to regenerate the file every few hours. And that&#8217;s how I used to run all of my planets. But, recently, I&#8217;ve moved to running them on GitHub Pages instead. And that&#8217;s what we&#8217;ll look at next.</p>
<p>There are two parts to this. We need to configure our repo to have a GitHub Pages site associated with it and we also need to configure GitHub Actions to rebuild the site every few hours. Let&#8217;s take those two in turn.</p>
<p>Turning on GitHub Pages is simple enough. Just go to the &#8220;Pages&#8221; section in your repo&#8217;s settings. Choose &#8220;GitHub Actions&#8221; as the deployment source and tick the box marked &#8220;Enforce HTTPS&#8221;. Later on, you can look at setting up a custom domain for your site but, for now, let&#8217;s stick with the default URL which will be https://&lt;github_username&gt;.github.io/&lt;repo_name&gt;. Nothing will appear yet, as we need to set up GitHub Actions next.</p>
<p>Setting up a GitHub Action workflow is as simple as adding a YAML file to the /.github/workflows directory in your repo. You&#8217;ll obviously have to create that directory first. Here&#8217;s the workflow definition for Planet Perl (it&#8217;s in a file called &#8220;buildsite.yml&#8221;, but that name isn&#8217;t important).</p><pre class="urvanov-syntax-highlighter-plain-tag">name: Generate web page

on:
  push:
    branches: '*'
  schedule:
    - cron: '37 */4 * * *'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    container: davorg/perl-perlanet:latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Create pages
      run: |
        mkdir -p docs
        perlanet &gt; perlanet.log 2&gt;&amp;1
    - name: Commit new page
      if: github.repository == 'davorg/planetperl'
      run: |
        git config --global --add safe.directory /__w/planetperl/planetperl
        GIT_STATUS=$(git status --porcelain)
        echo $GIT_STATUS
        git config user.name github-actions[bot]
        git config user.email 41898282+github-actions[bot]@users.noreply.github.com
        git add docs/
        if [ "$GIT_STATUS" != "" ]; then git commit -m "Automated Web page generation"; fi
        if [ "$GIT_STATUS" != "" ]; then git push; fi
    - name: Archive perlanet logs
      uses: actions/upload-artifact@v3
      with:
        name: perlanet.log
        path: ./perlanet.log
        retention-days: 3

    - name: Update pages artifact
      uses: actions/upload-pages-artifact@v1
      with:
        path: docs/

  deploy:
    needs: build
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2</pre><p>The first section of the file defines the events that will trigger this workflow. I have defined three triggers:</p>
<ol>
<li>Pushing a commit. I could be cleverer here and only work when certain files are changed (for example, the config or the index.tt)</li>
<li>On a schedule. My example runs at 37 minutes past the hour every four hours (so at 04:37, 08:37, etc.)</li>
<li>Manually. The &#8220;workflow_dispatch&#8221; trigger adds a button to the repo&#8217;s &#8220;Actions&#8221; page on GitHub allowing you to run the workflow manually, whenever you want</li>
</ol>
<p>Following that, we define the jobs that need to be run and the steps that make up those jobs. We have two jobs &#8211; one that builds the new version of the site and one that deploys that new site to GitHub Pages. Remember how I mentioned earlier that there is a Perlanet container on the Docker Hub? Well, you&#8217;ll see that the build job runs on that container. This is because pulling a container from the Docker Hub is faster than using a standard Ubuntu container and installing Perlanet.</p>
<p>The steps in these jobs should be pretty self-explanatory. Basically, we check out the repo, run &#8220;perlanet&#8221; to build the site and then deploy the contents of the /docs directory to the GitHub Pages server.</p>
<p>Once you&#8217;ve created this file and added it to your repo, you&#8217;ll see details of this workflow on the &#8220;Actions&#8221; tab in your repo. And whenever you push a change or when a scheduled run takes place (or you press the manual run button) you&#8217;ll see logs for the run and (hopefully) your web site will update to contain the latest data.</p>
<p>I reckon you can get a new planet up and running in about half an hour. Oh, and if you label your repo with the topic &#8220;perlanet&#8221;, then it will automatically be added to The Planetarium.</p>
<p>So, what are you waiting for? What planet would you like to build?</p>
<p>The post <a rel="nofollow" href="https://perlhacks.com/2023/04/building-planets-with-perlanet-and-github/">Building Planets with Perlanet and GitHub</a> appeared first on <a rel="nofollow" href="https://perlhacks.com">Perl Hacks</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">perl hacks</span>

              <p class="text-right"><small>Published by Dave Cross
     on Sunday 02 April 2023 17:54</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://dev.to/davorg/building-planets-with-perlanet-and-github-4jpe">Building Planets with Perlanet and GitHub</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><blockquote>
<p>Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles is an utterly insignificant little blue green planet whose ape-descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea.</p>

<p><em>Douglas Adams – The Hitchhiker’s Guide to the Galaxy</em></p>
</blockquote>

<p>I don’t still wear a digital watch, but I do like other things that are almost as unhip. In particular, I pine for the time about twenty years ago when web feeds looked like they were about to take over the world. Everyone had their favourite feed reader (I still miss Google Reader) and pretty much any useful web site would produce one or more web feeds that you could subscribe to and follow through your feed reader. For a few years, it was almost unthinkable to produce a web site without publishing a feed which included the changes to the site’s content.</p>

<p>Then, at some point, that changed. It wasn’t that web feeds vanished overnight. They still exist for many sites. But they are no longer ubiquitous. You can’t guarantee they’ll exist for every site you’re interested in. I remember people saying that social media would replace them. I was never convinced by that argument but, interestingly, one of the first times I noticed them vanishing was when Twitter removed their web feed of a user’s posts. They wanted people to use their AP instead (so I wrote <a href="https://github.com/davorg/twitter-json2atom">twitter-json2atom</a> that turned their API’s JSON into an Atom feed – I suspect it no longer works). Honestly, I think the main reason for the fall in popularity of web feeds was that people wanted you to read their content on their web sites where the interesting content was surrounded by uninteresting adverts.</p>

<p>But, as I said, not all web feeds vanished. There are still plenty of them out there (often, I expect because the sites’ owners don’t realise they’re there or don’t know how to turn them off). And that means the web feed-driven technologies of the early 2000s can still be useful.</p>

<p>One such piece of technology is the feed aggregator. I remember these being very popular. You would create a web site and configure it with a list of web feeds that you were interested in. The site would be driven by a piece of software that every few hours would poll the web feeds in the configuration and use the information it found to create a) a web page made up of information from the feeds and b) another feed that contained all of the information from the source feeds. The most popular software for building these sites was called Planet Planet and was written in Python (it seems to have vanished sometime in the last twenty years, otherwise I would link to it). When I wrote a Perl version, I called it (for reasons I now regret) <a href="https://metacpan.org/pod/Perlanet">Perlanet</a>.</p>

<p>I still use Perlanet to build planet sites. And they’re all listed at <a href="https://theplanetarium.org/">The Planetarium</a>. Recently, I’ve started hosting all my planets on <a href="https://pages.github.com/">GitHub Pages</a>, using <a href="https://github.com/features/actions">GitHub Actions</a> to rebuild the sites periodically. I thought that maybe other people might be old-skool like me and might want to build their own planets – so in the rest of this post I’ll explain how to do that, using <a href="https://perl.theplanetarium.org/">Planet Perl</a> as an example.</p>

<p>The first thing you’ll need is a GitHub account and a repo to store the code for your planet. I’m going to assume you know how to set those up (in the interest of keeping this tutorial short). You only actually need two files to create a planet – a config file and a template for the web site.</p>

<p>Here’s part of the config for Planet Perl:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>title: Planet Perl
description: There's More Than One Way To Aggregate It
url: https://perl.theplanetarium.org/
author:
  name: Dave Cross
  email: dave@theplanetarium.org
  twitter: davorg
entries: 75
entries_per_feed: 5
opml_file: docs/opml.xml
page:
  file: docs/index.html
  template: index.tt
feed:
  file: docs/atom.xml
  format: Atom
google_ga: G-HD966GMRYP
cutoff_duration:
  months: 1
feeds:
  - feed: https://www.perl.com/article/index.xml
    title: perl.com
    web: https://perl.com/
  - feed: https://news.perlfoundation.org/atom.xml
    title: Perl Foundation News
    web: https://news.perlfoundation.org/
</code></pre>

</div>



<p>I’ve tried to make it self-explanatory. At the top, there are various config options for the output (the web page and the aggregated feed) and, below, are details of the feeds that you want to aggregate. Let’s look at the output options first.</p>

<ul>
<li>title and description: these are both strings that you can include on the web page that is created. They’re also used in the aggregate feed that is produced</li>
<li>url: this is where the web page will be available on the web</li>
<li>author: this contains details of the person publishing the aggregated site and feed. The Twitter handle is optional</li>
<li>entries: is the maximum number of entries that your output will contain in total</li>
<li>entries_per_feed: is the maximum number of entries that you will use from each of your feeds. This is to stop your output being swamped with entries from a particularly busy feed. This can be omitted, in which case there will be no limit</li>
<li>opml_file: OPML stands for “Outline Processor Markup File”. It used to be trendy to publish an OPML file which is a machine-readable data file which contains a list of the feeds that you are aggregating. These days, no-one cares. If you omit this setting, the file won’t be created.</li>
<li>page: this contains details of the web page you create. The template is the name of a template file that is used to create the HTML page (more on that below) and file is where the output page is written. If you keep the value used in my example, then things will work well with GitHub Pages as we’ll see later</li>
<li>feed: this contains details of the aggregate feed we create. You can choose a format (Atom or RSS) and the filename. Again, the default filename will work well with GitHub Pages</li>
<li>google_ga: if this value exists, then it will be used as the Google Analytics identifier for the web page that is created</li>
<li>cutoff_duration: this is another way to control which entries are used in your output feed. Any entries that were published longer ago than this period of time will be ignored</li>
</ul>

<p>Then we have the section of the config file that defines the feeds that we are going to aggregate. Each feed has three data items:</p>

<ul>
<li>feed: the URL of the feed</li>
<li>title: a string to use to describe the feed</li>
<li>web: the URL of the feed’s original web page</li>
</ul>

<p>And that’s all you need for the config file. Create that, put it in a file called “perlanetrc” and add it to your repo.</p>

<p>The other file you need is the template for the HTML page. This is usually called “index.tt”. <a href="https://github.com/davorg/planetperl/blob/master/index.tt">The one I use for Planet Perl</a> is rather complicated (there are all sorts of Javascript tricks in it). <a href="https://github.com/davorg/planetdavorg/blob/main/index.tt">The one I use for Planet Davorg</a> is far simpler – and should work well with the config file above. I suggest going with that initially and editing it once you’ve got everything else working.</p>

<p>I said those are the only two files you need. And that’s true. But the site you create will be rather ugly. My default web page uses <a href="https://getbootstrap.com/">Bootstrap</a> for CSS, but you’ll probably want to add your own CSS to tweak the way it looks – along with, perhaps, some Javascript and some images. All of the files that you need to make your site work should be added to the /docs directory in your repo.</p>

<p>Having got to this stage, we can test your web site. Well, we’ll need to install Perlanet first. There are two ways to do this. You can either install it from CPAN along with all of its (many) dependencies – using “cpan Perlanet” or there’s <a href="https://hub.docker.com/r/davorg/perl-perlanet">a Docker image</a> that you can use. Either way, once you have the software installed, running it is as simple as running “perlanet”. That will trundle along for a while and, when it has finished, you’ll find new files called “index.html” and “atom.xml” in the /docs directory. My favourite way to test the output locally is to use <a href="https://metacpan.org/dist/App-HTTPThis">App::HTTPThis</a>. Having installed this program, you can just run “http_this docs” from the repo’s main directory and then visit <a href="http://localhost:7007/index.html">http://localhost:7007/index.html</a> to see the site that was produced (or <a href="http://localhost:7007/atom.xml">http://localhost:7007/atom.xml</a> to see the feed.</p>

<p>You now have a system to build your new planet. You could run that on a server that’s connected to the internet and set up a cronjob to regenerate the file every few hours. And that’s how I used to run all of my planets. But, recently, I’ve moved to running them on GitHub Pages instead. And that’s what we’ll look at next.</p>

<p>There are two parts to this. We need to configure our repo to have a GitHub Pages site associated with it and we also need to configure GitHub Actions to rebuild the site every few hours. Let’s take those two in turn.</p>

<p>Turning on GitHub Pages is simple enough. Just go to the “Pages” section in your repo’s settings. Choose “GitHub Actions” as the deployment source and tick the box marked “Enforce HTTPS”. Later on, you can look at setting up a custom domain for your site but, for now, let’s stick with the default URL which will be https://.github.io/. Nothing will appear yet, as we need to set up GitHub Actions next.</p>

<p>Setting up a GitHub Action workflow is as simple as adding a YAML file to the /.github/workflows directory in your repo. You’ll obviously have to create that directory first. Here’s the workflow definition for Planet Perl (it’s in a file called “buildsite.yml”, but that name isn’t important).<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>name: Generate web page

on:
  push:
    branches: '*'
  schedule:
    - cron: '37 */4 * * *'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    container: davorg/perl-perlanet:latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Create pages
      run: |
        mkdir -p docs
        perlanet &gt; perlanet.log 2&gt;&amp;1
    - name: Commit new page
      if: github.repository == 'davorg/planetperl'
      run: |
        git config --global --add safe.directory /__w/planetperl/planetperl
        GIT_STATUS=$(git status --porcelain)
        echo $GIT_STATUS
        git config user.name github-actions[bot]
        git config user.email 41898282+github-actions[bot]@users.noreply.github.com
        git add docs/
        if ["$GIT_STATUS" != ""]; then git commit -m "Automated Web page generation"; fi
        if ["$GIT_STATUS" != ""]; then git push; fi
    - name: Archive perlanet logs
      uses: actions/upload-artifact@v3
      with:
        name: perlanet.log
        path: ./perlanet.log
        retention-days: 3

    - name: Update pages artifact
      uses: actions/upload-pages-artifact@v1
      with:
        path: docs/

  deploy:
    needs: build
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v2
</code></pre>

</div>



<p>The first section of the file defines the events that will trigger this workflow. I have defined three triggers:</p>

<ol>
<li>Pushing a commit. I could be cleverer here and only work when certain files are changed (for example, the config or the index.tt)</li>
<li>On a schedule. My example runs at 37 minutes past the hour every four hours (so at 04:37, 08:37, etc.)</li>
<li>Manually. The “workflow_dispatch” trigger adds a button to the repo’s “Actions” page on GitHub allowing you to run the workflow manually, whenever you want</li>
</ol>

<p>Following that, we define the jobs that need to be run and the steps that make up those jobs. We have two jobs – one that builds the new version of the site and one that deploys that new site to GitHub Pages. Remember how I mentioned earlier that there is a Perlanet container on the Docker Hub? Well, you’ll see that the build job runs on that container. This is because pulling a container from the Docker Hub is faster than using a standard Ubuntu container and installing Perlanet.</p>

<p>The steps in these jobs should be pretty self-explanatory. Basically, we check out the repo, run “perlanet” to build the site and then deploy the contents of the /docs directory to the GitHub Pages server.</p>

<p>Once you’ve created this file and added it to your repo, you’ll see details of this workflow on the “Actions” tab in your repo. And whenever you push a change or when a scheduled run takes place (or you press the manual run button) you’ll see logs for the run and (hopefully) your web site will update to contain the latest data.</p>

<p>I reckon you can get a new planet up and running in about half an hour. Oh, and if you label your repo with the topic “perlanet”, then it will automatically be added to The Planetarium.</p>

<p>So, what are you waiting for? What planet would you like to build?</p>

<p>The post <a href="https://perlhacks.com/2023/04/building-planets-with-perlanet-and-github/">Building Planets with Perlanet and GitHub</a> appeared first on <a href="https://perlhacks.com">Perl Hacks</a>.</p>

       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">dev.to</span>

              <p class="text-right"><small>Published by Dave Cross
     on Sunday 02 April 2023 17:54</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://dev.to/davorg/deploying-github-pages-site-with-github-workflows-3bhh">Deploying GitHub Pages sites with GitHub Workflows</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>I've written before about <a href="https://dev.to/davorg/github-actions-for-semi-static-web-sites-597g">how I use GitHub Workflows to keep "semi-static" web sites up to date</a>. It's a technique that I've found really useful. When I wrote that blog post, things were pretty simple - you chose which branch held your web site (there was a tradition for a while to use <code>gh-pages</code>) and whether the web site pages were in the repos root directory or the directory called <code>/docs</code>. I usually put my web site files into the <code>/docs</code> directory in the <code>master</code> (now <code>main</code>) branch and things worked just fine.</p>

<p>The reason for storing the site in <code>/docs</code> was so that there was a separation between the files that were used to generate the site from the generated output site itself. Many of my repos would have a <code>/tt</code> directory that contained templates, a <code>/data</code> directory which contains JSON files or an SQLite database and a <code>/bin</code> directory with a <code>build</code> program that pulls all that stuff together and generates a pile of HTML files that end up in the <code>/docs</code> directory. In my original blog post on this subject, I demonstrated a GitHub Workflow definition that would regenerate the site (when input files changed or on a schedule) and committed any changed files in the <code>/docs</code> directory. Some GitHub magic would then ensure that the new version of the site was deployed to the GitHub Pages server. All was well with the world.</p>

<p>Then, a few months ago, things got a little more complicated. We gained options about how your GitHub Pages site was deployed. The standard version that I'd be using before was called "deploy from a branch" but there was another option called "GitHub Actions". It seemed likely to me that I really needed to start using the "GitHub Actions" option, but things were still working the old way, and I had far more interesting things to investigate, so I left things the way they were.</p>

<p>Well, I say things were still working in the old way... They were, but something was a bit different. It seemed that the old method was being powered by a new GitHub Workflow called "pages-build-deployment" that had been automatically added to all the repos that needed it. And looking into the details of that workflow, I noticed that it was doing some things that were unnecessary in my repos - for example it assumed that the site was being built using <a href="https://jekyllrb.com/">Jekyll</a> and that was only true for a couple of my repos. For most of them, that was unnecessary work. So I needed to look into the new deployment option in more detail.</p>

<p>I started a couple of weeks ago, by simply switching the option from "deploy from a branch" to "GitHub Actions" in the hope that, because I was already using GitHub Actions, things would Just Work. But, unfortunately, that wasn't the case. My new site was being generated and committed to the repo - but the changes weren't showing up on the live site. So I switched things back until I had time to look into in it more detail.</p>

<p>That time was today. It seemed that I needed to include code in my GitHub Workflow that would actually handle the deployment of the site to the GitHub Pages servers. A quick search of the <a href="https://github.com/marketplace?type=actions">GitHub Actions marketplace</a> found the <a href="https://github.com/marketplace/actions/deploy-github-pages-site">Deploy GitHub Pages site</a> action which seemed to be the right thing. But reading the documentation, I worked out that it wanted to deploy the site from an artifact, so I needed to create that first. And then I found <a href="https://github.com/marketplace/actions/upload-github-pages-artifact">Upload GitHub Pages artifact</a> which did the right thing. So it was just a case of adding these two actions to my workflows in the correct way.</p>

<p>Previously, my workflows for these sites just needed a single job (called <code>build</code>) but now I added a <code>deploy</code> job which depended on <code>build</code>. For example, the workflow that builds <a href="https://perl.theplanetarium.org/">Planet Perl</a> now looks like this:</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>name: Generate web page

on:
  push:
    branches: '*'
  schedule:
    - cron: '37 */4 * * *'
  workflow_dispatch:

jobs:
  build:
    runs-on: ubuntu-latest
    container: davorg/perl-perlanet:latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Create pages
      run: |
        mkdir -p docs
        perlanet &gt; perlanet.log 2&gt;&amp;1
    - name: Commit new page
      if: github.repository == 'davorg/planetperl'
      run: |
        git config --global --add safe.directory /__w/planetperl/planetperl
        GIT_STATUS=$(git status --porcelain)
        echo $GIT_STATUS
        git config user.name github-actions[bot]
        git config user.email 41898282+github-actions[bot]@users.noreply.github.com
        git add docs/
        if [ "$GIT_STATUS" != "" ]; then git commit -m "Automated Web page generation"; fi
        if [ "$GIT_STATUS" != "" ]; then git push; fi
    - name: Archive perlanet logs
      uses: actions/upload-artifact@v3
      with:
        name: perlanet.log
        path: ./perlanet.log
        retention-days: 3

    - name: Update pages artifact
      uses: actions/upload-pages-artifact@v1
      with:
        path: docs/

  deploy:
    needs: build
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${\{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    steps:
    - name: Deploy to GitHub Pages
      id: deployment
      uses: actions/deploy-pages@v1
</code></pre>

</div>

<p>The bits that I've added are the final step in the <code>build</code> job ("Update pages artifact") and the new <code>deploy</code> job. All of the code is largely copied from the documentation of the two actions I mentioned above.</p>

<p>Having made this changes to one of my planet sites, I switched the deployment method and forced the workflow to run. And was very happy to see it ran successfully and the new version of the site appeared at the live URL as soon as the deployment had changed.</p>

<p>This makes me happy as I feel I'm using the GitHub Pages deployment the way that they're supposed to be used. I've updated all of my <a href="https://theplanetarium.org/">planet sites</a> to use this method, but I have several other sites that I'll need to get round to switching at some point.</p>

<p>As always when I find out something new about a GitHub feature, it leaves me with a couple of other suggestions for improvements:</p>

<ul>
<li>It's possible to <a href="https://docs.github.com/en/actions/using-workflows/reusing-workflows">call one workflow from another</a>. The planet workflows are all very similar. I wonder if I can define a single workflow that does all of the work and just call that from the individual workflow definition - passing in parameters to handle the differences.</li>
<li>Now I'm deploying the sites from artifacts, there is no need for the generated site to actually exist in the repo. That might well make a few things quite a bit easier.</li>
</ul>

<p>Anyway, I thought I'd share what I had discovered today. Is anyone else generating web sites this way? How do you do it?</p>

       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">dev.to</span>

              <p class="text-right"><small>Published by Dave Cross
     on Monday 20 March 2023 17:32</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://perlhacks.com/2023/03/improvements-to-planet-perl-and-perlanet/">Improvements to Planet Perl and Perlanet</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>This is a story of one of those nice incidents where something starts off simple, then spirals out of control for a while but, in the end, everyone wins.</p>
<p>On Reddit, a few days ago, someone asked <a href="https://www.reddit.com/r/perl/comments/119hu00/perl_rss_feeds/">&#8216;Is there a &#8220;Planet Perl&#8221; with an RSS feed?&#8217;</a> and a few people replied, pointing out the existence of Planet Perl (which is the first Google result for <a href="https://www.google.com/search?q=planet+perl">&#8220;Planet Perl&#8221;</a>). I&#8217;m obviously not marketing that site very well as every time I mention it, I get people (pleasantly) surprised that it exists.</p>
<p>On this occasion, it was <a href="https://www.reddit.com/user/rwp0/">Elvin Aslanov</a> who seemed to discover my site for the first time. And, very soon afterwards, he started sending <a href="https://github.com/davorg/planetperl/pulls">pull requests</a> to add feeds to the site. As a result, we now have three more feeds that are being pulled into the site.</p>
<ul>
<li><a href="https://medium.com/tag/perl">Perl on Medium</a>. I&#8217;m slightly embarrassed that I hadn&#8217;t thought of this myself. I did, after all, once try to start <a href="https://medium.com/cultured-perl">a Perl publication on Medium</a>. I think I must have decided that there are better sites for technical blogging and blanked it from consideration. Medium&#8217;s not the busiest of places for Perl bloggers, but there are a few posts there and they&#8217;re mostly from people who are outside of the echo chamber &#8211; so getting more eyes on their posts is a good idea.</li>
<li><a href="https://stackoverflow.com/feeds/tag/perl">Perl questions on Stack Overflow</a>. Another one that would have been obvious if I had thought for a second. I&#8217;ve been answering questions on SO for years. It&#8217;s a good way to get more perspective on how Perl is being used across the industry. Unfortunately, the feed only includes the titles of the posts &#8211; you&#8217;ll need to click the link to actually see the question.</li>
<li><a href="https://github.com/Perl/perl5/commits/blead.atom">Perl commits on GitHub</a>. I&#8217;m interested in hearing how useful people think this is. I worry slightly that there will be times when the number of commits will overwhelm the other feeds. But maybe that&#8217;s a good idea. Perhaps it&#8217;s good for more people to see just how busy the Perl 5 Porters are. I&#8217;m a bit annoyed that the feed puts everything in a fixed-width font, but not (yet) annoyed enough to do anything about it.</li>
</ul>
<p>You might know that Planet Perl is driven by <a href="https://metacpan.org/pod/Perlanet">Perlanet</a>. So adding new feeds is just a case of adding a few lines to <a href="https://github.com/davorg/planetperl/blob/master/perlanetrc">a configuration file</a>. And looking at the pull requests I got from Elvin, showed a potential problem in the way the configuration was laid out. Each feed has three lines of YAML configuration. There&#8217;s a title for the feed, a URL for a web page that displays the content of the feed and the URL for the feed itself. They&#8217;re called &#8220;title&#8221;, &#8220;web&#8221; and &#8220;url&#8221;. And it&#8217;s that last name that&#8217;s slightly problematic &#8211; it&#8217;s just not clear enough. Elvin got &#8220;web&#8221; and &#8220;url&#8221; muddled up in one of his PRs and, when I pointed that out to him, he suggested that renaming &#8220;url&#8221; to &#8220;feed&#8221; would make things much clearer.</p>
<p>I agreed, and the next day I hacked away for a while before releasing <a href="https://metacpan.org/release/DAVECROSS/Perlanet-v3.0.0/view/lib/Perlanet.pm">version 3.0.0 of Perlanet</a>. In this version, the &#8220;url&#8221; key is renamed to &#8220;feed&#8221;. It still accepts the old name (so older config files will still work) but you&#8217;ll get a warning if you try to use a config name in the old config.</p>
<p>I didn&#8217;t stop there. Last year, I wrote <a href="https://perlhacks.com/2022/10/building-a-perlanet-container/">a blog post about producing a docker image that already had Perlanet installed</a> &#8211; so that it was quicker to rebuild my various planets every few hours. Since then I&#8217;ve been rebuilding <a href="https://hub.docker.com/repository/docker/davorg/perl-perlanet/general">that image</a> every time I updated Perlanet. But it&#8217;s been rather a manual process. And because I&#8217;m old and decrepit, I can never remember the steps I go through to rebuild it, tag it correctly and push it to the Docker Hub. This means it always takes far longer than it&#8217;s supposed to. So this time, I wrote <a href="https://github.com/davorg/perl-perlanet-docker/blob/main/build">a script to do that for me</a>. And because I now have the kind of mind set that sees GitHub Workflows everywhere I look, I wrote <a href="https://github.com/davorg/perl-perlanet-docker/blob/main/.github/workflows/publish_image.yml">a Workflow definition that builds and publishes the image</a> any time the Dockerfile changes. I guess the next step will be to write an action that automatically updates the Dockerfile (thereby triggering the rebuild) each time I release a new version of Perlanet.</p>
<p>But that&#8217;s a problem for another day. For now, I&#8217;m happy with the improvements I&#8217;ve made to Planet Perl, Perlanet and the Perlanet Docker infrastructure.</p>
<p>The post <a rel="nofollow" href="https://perlhacks.com/2023/03/improvements-to-planet-perl-and-perlanet/">Improvements to Planet Perl and Perlanet</a> appeared first on <a rel="nofollow" href="https://perlhacks.com">Perl Hacks</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">perl hacks</span>

              <p class="text-right"><small>Published by Dave Cross
     on Friday 03 March 2023 10:16</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://dev.to/davorg/improvements-to-planet-perl-and-perlanet-20d3">Improvements to Planet Perl and Perlanet</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>This is a story of one of those nice incidents where something starts off simple, then spirals out of control for a while but, in the end, everyone wins.</p>

<p>On Reddit, a few days ago, someone asked <a href="https://www.reddit.com/r/perl/comments/119hu00/perl_rss_feeds/">‘Is there a “Planet Perl” with an RSS feed?’</a> and a few people replied, pointing out the existence of Planet Perl (which is the first Google result for <a href="https://www.google.com/search?q=planet+perl">“Planet Perl”</a>). I’m obviously not marketing that site very well as every time I mention it, I get people (pleasantly) surprised that it exists.</p>

<p>On this occasion, it was <a href="https://www.reddit.com/user/rwp0/">Elvin Aslanov</a> who seemed to discover my site for the first time. And, very soon afterwards, he started sending <a href="https://github.com/davorg/planetperl/pulls">pull requests</a> to add feeds to the site. As a result, we now have three more feeds that are being pulled into the site.</p>

<ul>
<li>
<a href="https://medium.com/tag/perl">Perl on Medium</a>. I’m slightly embarrassed that I hadn’t thought of this myself. I did, after all, once try to start <a href="https://medium.com/cultured-perl">a Perl publication on Medium</a>. I think I must have decided that there are better sites for technical blogging and blanked it from consideration. Medium’s not the busiest of places for Perl bloggers, but there are a few posts there and they’re mostly from people who are outside of the echo chamber – so getting more eyes on their posts is a good idea.</li>
<li>
<a href="https://stackoverflow.com/feeds/tag/perl">Perl questions on Stack Overflow</a>. Another one that would have been obvious if I had thought for a second. I’ve been answering questions on SO for years. It’s a good way to get more perspective on how Perl is being used across the industry. Unfortunately, the feed only includes the titles of the posts – you’ll need to click the link to actually see the question.</li>
<li>
<a href="https://github.com/Perl/perl5/commits/blead.atom">Perl commits on GitHub</a>. I’m interested in hearing how useful people think this is. I worry slightly that there will be times when the number of commits will overwhelm the other feeds. But maybe that’s a good idea. Perhaps it’s good for more people to see just how busy the Perl 5 Porters are. I’m a bit annoyed that the feed puts everything in a fixed-width font, but not (yet) annoyed enough to do anything about it.</li>
</ul>

<p>You might know that Planet Perl is driven by <a href="https://metacpan.org/pod/Perlanet">Perlanet</a>. So adding new feeds is just a case of adding a few lines to <a href="https://github.com/davorg/planetperl/blob/master/perlanetrc">a configuration file</a>. And looking at the pull requests I got from Elvin, showed a potential problem in the way the configuration was laid out. Each feed has three lines of YAML configuration. There’s a title for the feed, a URL for a web page that displays the content of the feed and the URL for the feed itself. They’re called “title”, “web” and “url”. And it’s that last name that’s slightly problematic – it’s just not clear enough. Elvin got “web” and “url” muddled up in one of his PRs and, when I pointed that out to him, he suggested that renaming “url” to “feed” would make things much clearer.</p>

<p>I agreed, and the next day I hacked away for a while before releasing <a href="https://metacpan.org/release/DAVECROSS/Perlanet-v3.0.0/view/lib/Perlanet.pm">version 3.0.0 of Perlanet</a>. In this version, the “url” key is renamed to “feed”. It still accepts the old name (so older config files will still work) but you’ll get a warning if you try to use a config name in the old config.</p>

<p>I didn’t stop there. Last year, I wrote <a href="https://dev.to/davorg/building-a-perlanet-container-43cm">a blog post about producing a docker image that already had Perlanet installed</a> – so that it was quicker to rebuild my various planets every few hours. Since then I’ve been rebuilding <a href="https://hub.docker.com/repository/docker/davorg/perl-perlanet/general">that image</a> every time I updated Perlanet. But it’s been rather a manual process. And because I’m old and decrepit, I can never remember the steps I go through to rebuild it, tag it correctly and push it to the Docker Hub. This means it always takes far longer than it’s supposed to. So this time, I wrote <a href="https://github.com/davorg/perl-perlanet-docker/blob/main/build">a script to do that for me</a>. And because I now have the kind of mind set that sees GitHub Workflows everywhere I look, I wrote <a href="https://github.com/davorg/perl-perlanet-docker/blob/main/.github/workflows/publish_image.yml">a Workflow definition that builds and publishes the image</a> any time the Dockerfile changes. I guess the next step will be to write an action that automatically updates the Dockerfile (thereby triggering the rebuild) each time I release a new version of Perlanet.</p>

<p>But that’s a problem for another day. For now, I’m happy with the improvements I’ve made to Planet Perl, Perlanet and the Perlanet Docker infrastructure.</p>

<p>The post <a href="https://perlhacks.com/2023/03/improvements-to-planet-perl-and-perlanet/">Improvements to Planet Perl and Perlanet</a> appeared first on <a href="https://perlhacks.com">Perl Hacks</a>.</p>

       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">dev.to</span>

              <p class="text-right"><small>Published by Dave Cross
     on Friday 03 March 2023 10:16</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://dev.to/davorg/dependabot-and-github-actions-3lai">Dependabot and GitHub Actions</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>If you're using GitHub Workflows to automate bits of your development process (test runs, deployment, stuff like that) then you're almost certainly using GitHub Actions as well. Actions are pre-build chunks of functionality that you can use in your workflows and which save you having to write a load of code yourself. I've never, for example, written a workflow definition that doesn't use <a href="https://github.com/marketplace/actions/checkout">actions/checkout</a> to checkout the current repo on the container that is running that workflow. Part of my workflow definition will always look like this:</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code>steps:
  - name: Check out code
    uses: actions/checkout@v3
</code></pre>

</div>

<p>Or, at least, it will look a lot like that. The bit that might change is that <code>v3</code> on the end there. That is, of course, the version of the action that I'm using. And that will change when the author of the action releases a new version. Well, strictly speaking, it will change when I notice that there's a new version and bother to update the workflow definition.</p>

<p>Recently, I noticed that two of the actions I use frequently (including the checkout action which I use <em>everywhere</em>) had updated their version numbers and I hadn't known about the change. It took a few hours of work to update the version numbers in all of my workflow definitions.</p>

<p>And life isn't supposed to be that much work.</p>

<p>But there's a solution. And I discovered it thanks to <a href="https://www.reddit.com/r/github/comments/1134qvp/github_actions_is_it_safe_to_use_master/">a discussion on Reddit</a>.</p>

<p>If you're doing much work on GitHub (particularly in the Javascript/Node) area, then you'll have met <a href="https://github.com/dependabot">Dependabot</a>. It's a bot that scans GitHub repos for dependencies that are out of date and then (and here's the really clever bit!) it generates pull requests that update the repo to use the updated versions.</p>

<p>To be honest, I don't have many projects on GitHub that use the technologies that Dependabot targets by default. The few I have are web sites that use frameworks I've cloned from elsewhere - so I get the occasional PR that updates <code>packages.json</code> and I just apply them and forget about it.</p>

<p>But it seems that Dependabot can do far more than that. You just need to configure it properly in your repo. And one of the things it can do is to scan for out of date actions being used in your workflow definitions. You just need a file called <code>dependabot.yml</code> in your <code>.github</code> directory and the contents should look like this:</p>

<div class="highlight js-code-highlight">
<pre class="highlight plaintext"><code># Set update schedule for GitHub Actions

version: 2
updates:

  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      # Check for updates to GitHub Actions every week
      interval: "weekly"
</code></pre>

</div>

<p>Dependabot will then check your actions versions once a week and create PRs for any that are out of date. I know it works, because I added this to all of my repos this morning and it promptly created half a dozen PRs for repos where I had updated the versions but hadn't pushed the changes to GitHub. When I pushed the existing changes, Dependabot checked its PR against the new code, realised the PR was no longer needed and closed the PR. Which was nice :-)</p>

<p>It you're using GitHub Workflows (and I can't really think of any reason why you wouldn't be) then I recommend adding this file to your repos. Of course, GitHub has <a href="https://docs.github.com/en/code-security/dependabot/working-with-dependabot/keeping-your-actions-up-to-date-with-dependabot">documentation about this feature</a>.</p>

<p>Dependabot clearly has more uses than I thought it did. I'm going to have to investigate it further. I wonder if it can check CPAN dependencies.</p>

<p>Do you have any Dependabot tricks that I would find useful?</p>

       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">dev.to</span>

              <p class="text-right"><small>Published by Dave Cross
     on Thursday 16 February 2023 16:10</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://blog.dave.org.uk/2023/02/2022-in-gigs.html">2022 in Gigs</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body">
<p>Rather later than usual (again!) here is my review of the best ten gigs I saw in 2022. For the first time since 2019, I did actually see more than ten gigs in 2022 although my total of sixteen falls well short of my pre-pandemic years.</p>



<p>Here are my ten favourite gigs of the year. As always, they&#8217;re in chronological order.</p>



<ul>
<li><strong><a href="https://www.songkick.com/concerts/39759069-pale-waves-at-roundhouse">Pale Waves at the Roundhouse</a></strong><br>I&#8217;ve seen Pale Waves a few times now and I think they&#8217;ve firmly established their place on my &#8220;see them whenever they tour near me&#8221; list. This show was every bit as good as I&#8217;ve ever seen them.</li>



<li><strong><a href="https://www.songkick.com/concerts/39529194-orchestral-manoeuvres-in-the-dark-at-royal-albert-hall">Orchestral Manoeuvres in the Dark at the Royal Albert Hall</a></strong><br>Another band I see whenever I can. This was a slightly different set where the first half was called &#8220;Atmospheric&#8221; and concentrated on some deeper cuts from their back catalogue and the second half included all the hits.</li>



<li><strong><a href="https://www.songkick.com/concerts/39529194-orchestral-manoeuvres-in-the-dark-at-royal-albert-hall">Chvrches at Brixton Academy</a></strong><br>In 2020, I moved to a flat that&#8217;s about fifteen minutes&#8217; walk from Brixton Academy. But I had to wait about eighteen months in order to take advantage of that fact. The last couple of times I&#8217;ve seen Chvrches were at Alexandra Palace, so it was nice to see them at a smaller venue again. This show featured a not-entirely unexpected guest appearance from Robert Smith.</li>



<li><strong><a href="https://www.songkick.com/concerts/40060742-sunflower-bean-at-electric-ballroom">Sunflower Bean at Electric Ballroom</a></strong><br>Another act who I see live as often as I can. And this was a great venue to see them in.</li>



<li><strong><a href="https://www.songkick.com/concerts/39584368-pet-shop-boys-at-o2">Pet Shop Boys at the O2 Arena</a></strong><br>There&#8217;s always one show a year that draws me to the soulless barn that is the O2 Arena. Every time I go there, I vow it&#8217;ll be the last time &#8211; but something always pulls me back. This year it was the chance to see a band I loved in the 80s and have never seen live. This was a fabulous greatest hits show that had been postponed from 2020.</li>



<li><strong><a href="https://www.songkick.com/concerts/39894954-lorde-at-roundhouse">Lorde at the Roundhouse</a></strong><br>A new Lorde album means another Lorde tour. And, like Chvrches, she swapped the huge expanse of Alexandra Palace for multiple nights at a smaller venue. This was a very theatrical show that matched the vibe of the <em>Solar Power</em> album really well.</li>



<li><strong><a href="https://www.songkick.com/concerts/40274226-lcd-soundsystem-at-o2-academy-brixton">LCD Soundsystem at Brixton Academy</a></strong><br>Another show at Brixton Academy. For some reason, I didn&#8217;t know about this show until I walked past the venue a few days before and saw the &#8220;sold out&#8221; signs. But a day or so later, I got an email from the venue offering tickets. So I snapped one up and had an amazing evening. It was the first time I&#8217;d seen them, but I strongly suspect it won&#8217;t be the last. That&#8217;s them in the photo at the top of this post.</li>



<li><strong><a href="https://www.songkick.com/concerts/40320756-roxy-music-at-o2">Roxy Music at the O2 Arena</a></strong><br>Some years there are two shows that force me to the O2 Arena. And this was one of those years. I&#8217;ve been a fan of Roxy Music since the 70s but I&#8217;ve never seen them live. Honestly, it would have been better to have seen them in the 70s or 80s, but it was still a great show.</li>



<li><strong><a href="https://www.songkick.com/concerts/40313030-beabadoobee-at-o2-academy-brixton">Beabadoobee at Brixton Academy</a></strong><br>Sometimes you go to see an artist because of one song and it just works out. This was one of those nights. In fact, it turns out I didn&#8217;t actually know &#8220;Coffee For Your Head&#8221; very well &#8211; I just knew the sample that was used in another artist&#8217;s record. But this was a great night and I hope to see her again very soon.</li>



<li><strong><a href="https://www.songkick.com/concerts/40493394-sugababes-at-eventim-apollo">Sugababes at Eventim Apollo</a></strong><br>Another night of fabulous nostalgia. The Eventim Apollo seems to have become my venue of choice to see re-formed girl groups from the 80s and 90s &#8211; having seen Bananarama, All Saints and now The Sugababes there in recent years. They have a surprising number of hits (far more than I remembered before the show) and they put on a great show.</li>
</ul>



<p>Not everything could make the top ten though. I think was the first year that I saw Stealing Sheep and they didn&#8217;t make the list (their stage shows just get weirder and weirder and the Moth Club wasn&#8217;t a great venue for it) and I was astonished to find myself slightly bored at the Nine Inch Nails show at Brixton Academy.</p>



<p>A few shows sit just outside of the top ten &#8211; St. Vincent at the Eventim Apollo, John Grant at the Shepherd&#8217;s Bush Empire and Damon Albarn at the Barbican spring to mind.</p>



<p>But, all in all, it was a good year for live music and I&#8217;m looking forward to seeing more than sixteen shows this year.</p>



<p>Did you see any great shows this year? Tell us about them in the comments.</p>
<p>The post <a rel="nofollow" href="https://blog.dave.org.uk/2023/02/2022-in-gigs.html">2022 in Gigs</a> appeared first on <a rel="nofollow" href="https://blog.dave.org.uk">Davblog</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">davblog</span>

              <p class="text-right"><small>Published by Dave Cross
     on Friday 03 February 2023 16:32</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://dev.to/davorg/updating-github-pages-using-github-actions-395a">Updating GitHub Pages using GitHub Actions</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>I thought it might be fun to show how I spent an hour or so yesterday throwing together a simple web site that is hosted on <a href="https://pages.github.com/">GitHub Pages</a> and is updated daily using <a href="https://github.com/features/actions">GitHub Actions</a>.</p>

<h2>
  
  
  An itch to scratch
</h2>

<p>So many web sites start out with an itch to scratch, and this one is no different. In this case, it was about wanting to stay more informed.</p>

<p>Each day, the <a href="https://bbc.co.uk/news/">BBC News web site</a> publishes a story that shows the front pages of all of the British newspapers. Although a lot of the British press isn't particularly trustworthy, I still think it's good to get an overview of what they're saying about the day's news. So finding and reading this story on the BBC site is one of my morning rituals.</p>

<p>But they don't make it easy to find the archive of those stories. So it's hard to read anything than the current day's front pages - and even that story tends to vanish from the BBC site by lunchtime. I decided I'd like a page that contains an archive to these stories.</p>

<h2>
  
  
  Scraping the site
</h2>

<p>The BBC don't publish an API for their web site, so we need to resort to screen-scraping. That, of course makes the process inherently fragile but it seems to be the best we can do at this stage.</p>

<p>It wasn't hard to create a program that pulls what I want from the web site using <a href="https://metacpan.org/pod/Web::Query">Web::Query</a> (my tool of choice for scraping web sites).</p>

<p>Of course, having scraped the data, we need to store it somewhere. I decided to store it in a JSON document and worry about displaying it later.</p>

<p>So here's the code I wrote:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight perl"><code><span class="c1">#!/usr/bin/perl</span>

<span class="k">use</span> <span class="nv">strict</span><span class="p">;</span>
<span class="k">use</span> <span class="nv">warnings</span><span class="p">;</span>

<span class="k">use</span> <span class="nv">feature</span> <span class="p">'</span><span class="s1">say</span><span class="p">';</span>

<span class="k">use</span> <span class="nn">Time::</span><span class="nv">Piece</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">Web::</span><span class="nv">Query</span><span class="p">;</span>
<span class="k">use</span> <span class="nv">JSON</span><span class="p">;</span>

<span class="k">my</span> <span class="nv">$site</span>  <span class="o">=</span> <span class="p">'</span><span class="s1">https://bbc.co.uk</span><span class="p">';</span>
<span class="k">my</span> <span class="nv">$start</span> <span class="o">=</span> <span class="p">"</span><span class="si">$site</span><span class="s2">/news/</span><span class="p">";</span>
<span class="k">my</span> <span class="nv">$file</span>  <span class="o">=</span> <span class="p">'</span><span class="s1">docs/papers.json</span><span class="p">';</span>
<span class="k">my</span> <span class="nv">$js_p</span>  <span class="o">=</span> <span class="nv">JSON</span><span class="o">-&gt;</span><span class="k">new</span><span class="o">-&gt;</span><span class="nv">pretty</span><span class="o">-&gt;</span><span class="nv">canonical</span><span class="p">;</span>
<span class="k">my</span> <span class="nv">$data</span><span class="p">;</span>

<span class="k">if</span> <span class="p">(</span><span class="o">-</span><span class="nv">e</span> <span class="nv">$file</span><span class="p">)</span> <span class="p">{</span>
  <span class="nb">open</span> <span class="k">my</span> <span class="nv">$fh</span><span class="p">,</span> <span class="p">'</span><span class="s1">&lt;</span><span class="p">',</span> <span class="nv">$file</span> <span class="ow">or</span> <span class="nb">die</span> <span class="p">"</span><span class="s2">$!</span><span class="se">\n</span><span class="p">";</span>
  <span class="k">my</span> <span class="nv">$json</span> <span class="o">=</span> <span class="k">do</span> <span class="p">{</span> <span class="nb">local</span> <span class="vg">$/</span><span class="p">;</span> <span class="o">&lt;</span><span class="nv">$fh</span><span class="o">&gt;</span> <span class="p">};</span>
  <span class="nv">$data</span> <span class="o">=</span> <span class="nv">$js_p</span><span class="o">-&gt;</span><span class="nv">decode</span><span class="p">(</span><span class="nv">$json</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">my</span> <span class="nv">$start_len</span> <span class="o">=</span> <span class="nv">@$data</span><span class="p">;</span>

<span class="k">my</span> <span class="nv">$q</span> <span class="o">=</span> <span class="nv">wq</span><span class="p">(</span><span class="nv">$start</span><span class="p">);</span>

<span class="nv">$q</span><span class="o">-&gt;</span><span class="nv">find</span><span class="p">('</span><span class="s1">a</span><span class="p">')</span>
   <span class="o">-&gt;</span><span class="nb">each</span><span class="p">(</span><span class="k">sub </span><span class="p">{</span>
  <span class="k">my</span> <span class="p">(</span><span class="nv">$i</span><span class="p">,</span> <span class="nv">$elem</span><span class="p">)</span> <span class="o">=</span> <span class="nv">@_</span><span class="p">;</span>

  <span class="k">return</span> <span class="k">unless</span> <span class="nv">$elem</span><span class="o">-&gt;</span><span class="nv">text</span> <span class="o">=~</span> <span class="sr">/^The Papers:/</span><span class="p">;</span>

  <span class="nb">push</span> <span class="nv">@$data</span><span class="p">,</span> <span class="p">{</span>
    <span class="s">date</span> <span class="o">=&gt;</span> <span class="nb">localtime</span><span class="o">-&gt;</span><span class="nv">strftime</span><span class="p">('</span><span class="s1">%Y-%m-%d</span><span class="p">'),</span>
    <span class="s">text</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="nv">$elem</span><span class="o">-&gt;</span><span class="nv">text</span> <span class="o">=~</span> <span class="sr">s/^The Papers:\s*//r</span><span class="p">),</span>
    <span class="s">link</span> <span class="o">=&gt;</span> <span class="nv">$site</span> <span class="o">.</span> <span class="nv">$elem</span><span class="o">-&gt;</span><span class="nv">attr</span><span class="p">('</span><span class="s1">href</span><span class="p">'),</span>
  <span class="p">};</span>
<span class="p">});</span>

<span class="k">if</span> <span class="p">(</span><span class="nv">@$data</span> <span class="o">==</span> <span class="nv">$start_len</span><span class="p">)</span> <span class="p">{</span>
  <span class="nb">warn</span> <span class="p">"</span><span class="s2">No new article found</span><span class="se">\n</span><span class="p">";</span>
<span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
  <span class="nb">open</span> <span class="k">my</span> <span class="nv">$fh</span><span class="p">,</span> <span class="p">'</span><span class="s1">&gt;</span><span class="p">',</span> <span class="nv">$file</span> <span class="ow">or</span> <span class="nb">die</span> <span class="p">"</span><span class="s2">$!</span><span class="se">\n</span><span class="p">";</span>
  <span class="k">print</span> <span class="nv">$fh</span> <span class="nv">$js_p</span><span class="o">-&gt;</span><span class="nv">encode</span><span class="p">(</span><span class="nv">$data</span><span class="p">);</span>
<span class="p">}</span>
</code></pre>

</div>



<p>Nothing too complex there. We look for all of the <code>&lt;a&gt;</code> tags in the page and ignore the ones that don't contain text starting with "The Papers:". We then extract the information we want (the text, the link and the date) and store that all in the JSON document.</p>

<p>At that point, I could run the code to create the JSON file. I then created the GitHub repository and turned on GitHub Pages for the repo. Once that was all working, I could browse to <a href="https://davorg.github.io/bbc_papers/papers.json">https://davorg.github.io/bbc_papers/papers.json</a> to see the JSON.</p>

<h2>
  
  
  Doing it every day
</h2>

<p>We need to run this code every day. That's simple enough with GitHub Actions. We simply add a workflow definition file to the repo. The workflow looks like this:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight yaml"><code><span class="na">name</span><span class="pi">:</span> <span class="s">Overnight processing</span>

<span class="na">on</span><span class="pi">:</span>
  <span class="na">schedule</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">cron</span><span class="pi">:</span> <span class="s1">'</span><span class="s">0</span><span class="nv"> </span><span class="s">9</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*</span><span class="nv"> </span><span class="s">*'</span>
  <span class="na">workflow_dispatch</span><span class="pi">:</span>

<span class="na">jobs</span><span class="pi">:</span>
  <span class="na">build</span><span class="pi">:</span>
    <span class="na">runs-on</span><span class="pi">:</span> <span class="s">ubuntu-latest</span>

    <span class="na">steps</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Checkout</span>
      <span class="na">uses</span><span class="pi">:</span> <span class="s">actions/checkout@v3</span>

    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Install cpanm</span>
      <span class="na">run</span><span class="pi">:</span> <span class="s">sudo apt update &amp;&amp; sudo apt install cpanminus</span>

    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Install dependencies</span>
      <span class="na">run</span><span class="pi">:</span> <span class="s">sudo cpanm -n --installdeps .</span>

    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Add data</span>
      <span class="na">run</span><span class="pi">:</span> <span class="s">./get_link</span>

    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">Commit new page</span>
      <span class="na">run</span><span class="pi">:</span> <span class="pi">|</span>
        <span class="s">GIT_STATUS=$(git status --porcelain)</span>
        <span class="s">echo $GIT_STATUS</span>
        <span class="s">git config user.name github-actions[bot]</span>
        <span class="s">git config user.email 41898282+github-actions[bot]@users.noreply.github.com</span>
        <span class="s">git add docs/</span>
        <span class="s">if [ "$GIT_STATUS" != "" ]; then git commit -m "Overnight job"; fi</span>
        <span class="s">if [ "$GIT_STATUS" != "" ]; then git push; fi</span>
</code></pre>

</div>



<p>Again, this is all pretty much standard stuff. We run the workflow on a Ubuntu container. It's a three-phase process:</p>

<ol>
<li>Install everything we need</li>
<li>Run the program to get the new data</li>
<li>Commit the new data file to the repo</li>
</ol>

<p>Note that the workflow is triggered in two ways:</p>

<ul>
<li>It runs at 09:00 every morning</li>
<li>You can run it manually from the Actions tab in the repo (that's what the <code>workflow_dispatch</code> line does)</li>
</ul>

<h2>
  
  
  Displaying our wares
</h2>

<p>Having got a daily build of the data, we need to create a web page to display it. I'm not a web designer, so this is going to be necessarily basic. I threw together a simple page using <a href="https://getbootstrap.com/">Bootstrap</a>.</p>

<p>The next step was to grab the <code>papers.json</code> document, parse it and then display it on the page. Now, I can wrangle Javascript pretty successfully most of the time. but I wanted to get this working as quickly as possible, so I asked ChatGPT for some help. It only took a few iterations for it to give me this code:<br>
</p>

<div class="highlight js-code-highlight">
<pre class="highlight javascript"><code> <span class="nb">document</span><span class="p">.</span><span class="nx">addEventListener</span><span class="p">(</span><span class="dl">"</span><span class="s2">DOMContentLoaded</span><span class="dl">"</span><span class="p">,</span> <span class="kd">function</span><span class="p">(){</span>
    <span class="nx">fetch</span><span class="p">(</span><span class="dl">"</span><span class="s2">papers.json</span><span class="dl">"</span><span class="p">)</span>
    <span class="p">.</span><span class="nx">then</span><span class="p">(</span><span class="nx">response</span> <span class="o">=&gt;</span> <span class="nx">response</span><span class="p">.</span><span class="nx">json</span><span class="p">())</span>
    <span class="p">.</span><span class="nx">then</span><span class="p">(</span><span class="nx">data</span> <span class="o">=&gt;</span> <span class="p">{</span>
        <span class="kd">const</span> <span class="nx">tableBody</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">querySelector</span><span class="p">(</span><span class="dl">"</span><span class="s2">table tbody</span><span class="dl">"</span><span class="p">);</span>
        <span class="nx">data</span><span class="p">.</span><span class="nx">forEach</span><span class="p">(</span><span class="nx">item</span> <span class="o">=&gt;</span> <span class="p">{</span>
            <span class="kd">const</span> <span class="nx">row</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">createElement</span><span class="p">(</span><span class="dl">"</span><span class="s2">tr</span><span class="dl">"</span><span class="p">);</span>
            <span class="kd">const</span> <span class="nx">date</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">createElement</span><span class="p">(</span><span class="dl">"</span><span class="s2">td</span><span class="dl">"</span><span class="p">);</span>
            <span class="nx">date</span><span class="p">.</span><span class="nx">textContent</span> <span class="o">=</span> <span class="nx">item</span><span class="p">.</span><span class="nx">date</span><span class="p">;</span>
            <span class="nx">row</span><span class="p">.</span><span class="nx">appendChild</span><span class="p">(</span><span class="nx">date</span><span class="p">);</span>
            <span class="kd">const</span> <span class="nx">textLink</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">createElement</span><span class="p">(</span><span class="dl">"</span><span class="s2">td</span><span class="dl">"</span><span class="p">);</span>
            <span class="kd">const</span> <span class="nx">a</span> <span class="o">=</span> <span class="nb">document</span><span class="p">.</span><span class="nx">createElement</span><span class="p">(</span><span class="dl">"</span><span class="s2">a</span><span class="dl">"</span><span class="p">);</span>
            <span class="nx">a</span><span class="p">.</span><span class="nx">href</span> <span class="o">=</span> <span class="nx">item</span><span class="p">.</span><span class="nx">link</span><span class="p">;</span>
            <span class="nx">a</span><span class="p">.</span><span class="nx">textContent</span> <span class="o">=</span> <span class="nx">item</span><span class="p">.</span><span class="nx">text</span><span class="p">;</span>
            <span class="nx">textLink</span><span class="p">.</span><span class="nx">appendChild</span><span class="p">(</span><span class="nx">a</span><span class="p">);</span>
            <span class="nx">row</span><span class="p">.</span><span class="nx">appendChild</span><span class="p">(</span><span class="nx">textLink</span><span class="p">);</span>
            <span class="nx">tableBody</span><span class="p">.</span><span class="nx">appendChild</span><span class="p">(</span><span class="nx">row</span><span class="p">);</span>
        <span class="p">});</span>
    <span class="p">});</span>
<span class="p">});</span>
</code></pre>

</div>



<p>It didn't work first time. But that's because I'm an idiot and didn't tell ChatGPT the name of my JSON document or how to correctly identify the <code>&lt;table&gt;</code> element in the HTML. But once I'd corrected my errors it all worked perfectly.</p>

<h2>
  
  
  Summing up
</h2>

<p>So that's how I spent yesterday's lunch break. I can now see an archive of the BBC's stories about each day'd front pages by just going to:</p>

<ul>
<li><a href="https://davorg.github.io/bbc_papers/">https://davorg.github.io/bbc_papers/</a></li>
</ul>

<p>Of course, just after I'd written it, I (once again) had a look to see if someone else had created something similar and found the <a href="https://www.bbc.co.uk/news/topics/cpml2v678pxt">BBC's page</a> listing all of the stories. Ah well, I had fun putting my version together.</p>

<p>All of this code is available <a href="https://github.com/davorg/bbc_papers">on GitHub</a>.</p>

       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">dev.to</span>

              <p class="text-right"><small>Published by Dave Cross
     on Friday 13 January 2023 16:04</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://perlhacks.com/2022/12/writing-a-cpan-module-that-talks-to-chatgpt/">Writing a CPAN module that talks to ChatGPT</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p><a href="https://chat.openai.com/chat">ChatGPT</a> exposes an API, but there&#8217;s no CPAN module for taking advantage of that. Let&#8217;s put that right (with help from ChatGPT)&#8230;</p>
<hr />
<p><em>Write a Perl CPAN module for connecting to a ChatGPT server</em></p>
<p>To create a Perl CPAN module for connecting to a ChatGPT server, you will need to have Perl and the necessary dependencies installed on your system.</p>
<p>To begin, create a new directory for your module and create a file called &#8220;ChatGPT.pm&#8221; with the following content:</p><pre class="urvanov-syntax-highlighter-plain-tag">package ChatGPT;

use strict;
use warnings;

use Net::WebSocket::Client;
use Net::WebSocket::Server;
use HTTP::Async;

sub new {
  my ($class, %params) = @_;
  my $self = {
    host =&gt; $params{host},
    port =&gt; $params{port},
  };

  bless $self, $class;

  return $self;
}

sub send_prompt {
  my ($self, $prompt) = @_;

  my $http = HTTP::Async-&gt;new;
  my $client = Net::WebSocket::Client-&gt;new(
    url =&gt; "ws://$self-&gt;{host}:$self-&gt;{port}"
  );

  $client-&gt;connect;
  $client-&gt;send_utf8($prompt);

  my $response = $client-&gt;recv;

  $client-&gt;disconnect;

  return $response;
}

1;</pre><p>To use this module in your Perl code, you can create a new ChatGPT object and call the <code>send_prompt</code> method like this:</p><pre class="urvanov-syntax-highlighter-plain-tag">use ChatGPT;

my $chatgpt = ChatGPT-&gt;new(host =&gt; 'localhost', port =&gt; 8080);
my $response = $chatgpt-&gt;send_prompt('What is your name?');
print "Response: $response\n";</pre><p>This will send the text prompt &#8220;What is your name?&#8221; to the ChatGPT server running on localhost at port 8080, and print the response to the console.</p>
<p>Note: This is just a basic example of how you can create a Perl CPAN module for connecting to a ChatGPT server. There are many other features and considerations that you may want to add to your module, such as error handling and customization options.</p>
<p>To make your module available on CPAN, you will need to follow the guidelines for creating and distributing CPAN modules, which can be found in the Perl documentation and online resources.</p>
<p><em>Use Moose to create the class</em></p>
<p>To use Moose to create the <code>ChatGPT</code> class, you will need to have Moose installed on your system. You can install Moose by running the following command:</p><pre class="urvanov-syntax-highlighter-plain-tag">cpanm Moose</pre><p>With Moose installed, you can modify the class like this:</p><pre class="urvanov-syntax-highlighter-plain-tag">package ChatGPT;

use Moose;
use Net::WebSocket::Client;
use Net::WebSocket::Server;
use HTTP::Async;

has host =&gt; (
  is =&gt; 'ro',
  isa =&gt; 'Str',
  required =&gt; 1,
);

has port =&gt; (
  is =&gt; 'ro',
  isa =&gt; 'Int', required =&gt; 1,
);

sub send_prompt {
  my ($self, $prompt) = @_;

  my $http = HTTP::Async-&gt;new;
  my $client = Net::WebSocket::Client-&gt;new(
    url =&gt; "ws://$self-&gt;{host}:$self-&gt;{port}"
  );

  $client-&gt;connect; $client-&gt;send_utf8($prompt);
  my $response = $client-&gt;recv;
  $client-&gt;disconnect;

  return $response;
}

1;</pre><p>In this version of the class, the <code>host</code> and <code>port</code> attributes are defined using Moose&#8217;s <code>has</code> keyword. These attributes are required and must be specified when creating a new <code>ChatGPT</code> object. The <code>send_prompt</code> method is unchanged from the previous example.</p>
<p>To create a new <code>ChatGPT</code> object, you can use the <code>new</code> method provided by Moose like this:</p><pre class="urvanov-syntax-highlighter-plain-tag">use ChatGPT;

my $chatgpt = ChatGPT-&gt;new(host =&gt; 'localhost', port =&gt; 8080);

my $response = $chatgpt-&gt;send_prompt('What is your name?');

print "Response: $response\n";</pre><p>This will create a new <code>ChatGPT</code> object with the specified <code>host</code> and <code>port</code> attributes, and send the text prompt &#8220;What is your name?&#8221; to the ChatGPT server running on localhost at port 8080. The response will be printed to the console.</p>
<p>Note: This is just one way to use Moose to create the <code>ChatGPT</code> class. There are many other features and customization options available in Moose that you may want to consider adding to your class, such as type constraints, default values, and methods.</p>
<p><em>Call the class WebService::ChatGPT</em></p>
<p>To rename the <code>ChatGPT</code> class to <code>WebService::ChatGPT</code>, you will need to update the package name at the top of the file and update the <code>use</code> statements to refer to the new package name.</p>
<p>Here is how you can update the <code>ChatGPT</code> class to use the new package name:</p><pre class="urvanov-syntax-highlighter-plain-tag">package WebService::ChatGPT;

use Moose;
use Net::WebSocket::Client;
use Net::WebSocket::Server;

use HTTP::Async;

has host =&gt; (
  is =&gt; 'ro',
  isa =&gt; 'Str',
  required =&gt; 1,
);

has port =&gt; (
  is =&gt; 'ro',
  isa =&gt; 'Int',
  required =&gt; 1,
);

sub send_prompt {
  my ($self, $prompt) = @_;

  my $http = HTTP::Async-&gt;new;
  my $client = Net::WebSocket::Client-&gt;new(
    url =&gt; "ws://$self-&gt;{host}:$self-&gt;{port}"
  );

  $client-&gt;connect; $client-&gt;send_utf8($prompt);
  my $response = $client-&gt;recv;
  $client-&gt;disconnect;

  return $response;
}

1;</pre><p>With these changes, the class is now called <code>WebService::ChatGPT</code> and can be used like this:</p><pre class="urvanov-syntax-highlighter-plain-tag">use WebService::ChatGPT;

my $chatgpt = WebService::ChatGPT-&gt;new(host =&gt; 'localhost', port =&gt; 8080);

my $response = $chatgpt-&gt;send_prompt('What is your name?');
print "Response: $response\n";</pre><p>This will create a new <code>WebService::ChatGPT</code> object with the specified <code>host</code> and <code>port</code> attributes, and send the text prompt &#8220;What is your name?&#8221; to the ChatGPT server running on localhost at port 8080. The response will be printed to the console.,/p&gt;</p>
<p>Note: This is just one way to rename the <code>ChatGPT</code> class to <code>WebService::ChatGPT</code>. You may also need to update any other references to the class name in your code and documentation, as well as update the module file name and directory structure to reflect the new package name.</p>
<p><em>Suggest some tests using the standard Perl testing framework</em></p>
<p>Here are some examples of tests that you could write for the <code>WebService::ChatGPT</code> module using the standard Perl testing framework (Test::More):</p><pre class="urvanov-syntax-highlighter-plain-tag">use Test::More;
use WebService::ChatGPT;

# Test creating a new ChatGPT object
my $chatgpt = WebService::ChatGPT-&gt;new(host =&gt; 'localhost', port =&gt; 8080);
isa_ok($chatgpt, 'WebService::ChatGPT', 'Object is a ChatGPT');

# Test sending a prompt and receiving a response
my $response = $chatgpt-&gt;send_prompt('What is your name?');
is($response, 'My name is ChatGPT.', 'Response is correct');

# Test sending an invalid prompt
eval { $chatgpt-&gt;send_prompt('') };
like($@, qr/Invalid prompt/, 'Invalid prompt error is thrown');

done_testing();</pre><p>The first test uses the <code>isa_ok</code> function to check that the object created by the <code>new</code> method is an instance of the <code>WebService::ChatGPT</code> class.</p>
<p>The second test uses the <code>is</code> function to compare the expected response to the actual response returned by the <code>send_prompt</code> method.</p>
<p>The third test uses the <code>eval</code> function to catch any exceptions thrown by the <code>send_prompt</code> method when an invalid prompt is provided, and the <code>like</code> function to check that the exception message matches the expected error message.</p>
<p>These tests can be run using the <code>prove</code> command:</p><pre class="urvanov-syntax-highlighter-plain-tag">prove -l t/chatgpt.t</pre><p></p>
<hr />
<p>I haven&#8217;t uploaded it to CPAN yet. Do you think I should?</p>
<p>The post <a rel="nofollow" href="https://perlhacks.com/2022/12/writing-a-cpan-module-that-talks-to-chatgpt/">Writing a CPAN module that talks to ChatGPT</a> appeared first on <a rel="nofollow" href="https://perlhacks.com">Perl Hacks</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">perl hacks</span>

              <p class="text-right"><small>Published by Dave Cross
     on Tuesday 20 December 2022 16:12</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://davorg.medium.com/5-reasons-why-using-ai-to-generate-blog-posts-can-destroy-your-seo-f8db298de65b?source=rss-3781feb8a4f4------2">5 Reasons Why Using AI to Generate Blog Posts Can Destroy Your SEO</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><div class="medium-feed-item"><p class="medium-feed-image"><a href="https://davorg.medium.com/5-reasons-why-using-ai-to-generate-blog-posts-can-destroy-your-seo-f8db298de65b?source=rss-3781feb8a4f4------2"><img src="https://cdn-images-1.medium.com/max/1568/0*g61hVmjPNir439PR.jpg" width="1568"></a></p><p class="medium-feed-snippet">Using artificial intelligence (AI) to generate blog posts can be bad for search engine optimization (SEO) for several reasons.</p><p class="medium-feed-link"><a href="https://davorg.medium.com/5-reasons-why-using-ai-to-generate-blog-posts-can-destroy-your-seo-f8db298de65b?source=rss-3781feb8a4f4------2">Continue reading on Medium »</a></p></div>       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">medium</span>

              <p class="text-right"><small>Published by Dave Cross
     on Sunday 11 December 2022 11:39</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://blog.dave.org.uk/2022/12/5-reasons-why-using-ai-to-generate-blog-posts-can-destroy-your-seo.html">5 Reasons Why Using AI to Generate Blog Posts Can Destroy Your SEO</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body">
<p>Using artificial intelligence (AI) to generate blog posts can be bad for search engine optimization (SEO) for several reasons.</p>



<p>First and foremost, AI-generated content is often low quality and lacks the depth and substance that search engines look for when ranking content. Because AI algorithms are not capable of understanding the nuances and complexities of human language, the content they produce is often generic, repetitive, and lacks originality. This can make it difficult for search engines to understand the context and relevance of the content, which can negatively impact its ranking.</p>



<p>Additionally, AI-generated content is often not well-written or structured, which can make it difficult for readers to understand and engage with. This can lead to a high bounce rate (the percentage of visitors who leave a website after only viewing one page), which can also hurt the website&#8217;s ranking.</p>



<p>Furthermore, AI-generated content is often not aligned with the website&#8217;s overall content strategy and goals. Because AI algorithms are not capable of understanding the website&#8217;s target audience, brand voice, and core messaging, the content they produce may not be relevant or useful to the website&#8217;s visitors. This can lead to a poor user experience, which can also hurt the website&#8217;s ranking.</p>



<p>Another issue with AI-generated content is that it can be seen as spammy or low quality by both search engines and readers. Because AI-generated content is often produced in large quantities and lacks originality, it can be seen as an attempt to manipulate search engine rankings or trick readers into engaging with the website. This can lead to the website being penalized by search engines or losing the trust and loyalty of its visitors.</p>



<p>In conclusion, using AI to generate blog posts can be bad for SEO for several reasons. AI-generated content is often low quality, poorly written, and not aligned with the website&#8217;s content strategy. It can also be seen as spammy or low quality by both search engines and readers, which can hurt the website&#8217;s ranking and reputation. It is important for websites to prioritize creating high-quality, original, and relevant content to improve their SEO and provide a positive user experience.</p>



<p>[This post was generated using <a href="https://chat.openai.com/">ChatGPT</a>]</p>
<p>The post <a rel="nofollow" href="https://blog.dave.org.uk/2022/12/5-reasons-why-using-ai-to-generate-blog-posts-can-destroy-your-seo.html">5 Reasons Why Using AI to Generate Blog Posts Can Destroy Your SEO</a> appeared first on <a rel="nofollow" href="https://blog.dave.org.uk">Davblog</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">davblog</span>

              <p class="text-right"><small>Published by Dave Cross
     on Sunday 11 December 2022 11:38</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://davorg.medium.com/containers-for-coverage-1db8dfc76c2d?source=rss-3781feb8a4f4------2">Containers for Coverage</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*YjwJ59dr4XVoMb1b.png" /></figure><p>I’ve been building Docker containers again. And I think you’ll find this one a little more useful than <a href="https://perlhacks.com/2022/10/building-a-perlanet-container/">the Perlanet one</a> I wrote about a couple of weeks ago.</p><p>Several years ago I got into <a href="https://www.travis-ci.com/">Travis CI</a> and set up lots of <a href="https://github.com/davorg?tab=repositories">my GitHub repos</a> so they automatically ran the tests each time I committed to the repo. Later on, I also worked out how to tie those test runs into <a href="https://coveralls.io/">Coveralls.io</a> so I got pretty graphs of how my test coverage was looking. I gave <a href="https://www.slideshare.net/davorg/github-travisci-and-perl">a talk about what I had done</a>.</p><p>But two things changed.</p><p>Firstly, Travis CI got too popular and, eventually, removed their free service. And, secondly, <a href="https://github.com/features/actions">GitHub Actions</a> was introduced. Over the last few years, I’ve set up many of my repos to use GitHub Actions for CI. But, basically because I’m lazy, I didn’t remove the Travis CI configuration from those repos.</p><p>But last week I decided the time was right to start work on that. And when I went to remove the .travis.yml I realised that something was missing from my GitHub Actions CI workflows — they were running the unit tests, but they weren’t reporting on test coverage. So it was time to fix that.</p><p>I needed to reimplement the logic that connected Travis CI to Coveralls.io in a GitHub workflow. That actually turned out to be pretty simple. There’s a CPAN module called <a href="https://metacpan.org/pod/Devel::Cover::Report::Coveralls">Devel::Cover::Report::Coveralls</a> which takes the output from Devel::Cover, converts it to the correct format and sends it to Coveralls.io. And, as a bonus, it has documentation showing how to implement that in a GitHub workflow.</p><p>So I hacked at my workflow definition file for one of my CPAN modules and within a few minutes I had it working.</p><p>Well, I say “a few minutes”, but it took over thirteen minutes to run. It turns out that Devel::Cover::Report::Coveralls is a pretty heavyweight module and needs to install a lot of other modules in order to do its work.</p><p>At this point, you can probably guess where this is going. And you’d be right.</p><p>I’ve created a Docker container that has Devel::Cover::Report::Coveralls already installed. And, obviously, it’s available for everyone to use from the Docker hub — <a href="https://hub.docker.com/repository/docker/davorg/perl-coveralls">davorg/perl-coveralls</a>.</p><p>A couple of small adjustments to my GitHub workflow and the coverage job is now running on my new container — and takes 29 seconds instead of 13 minutes. So that’s a win.</p><p>The relevant section of my <a href="https://github.com/davorg/calendar-simple/blob/master/.github/workflows/perltest.yml">workflow file</a> is here:</p><pre>coverage:<br>    runs-on: ubuntu-latest<br>    container: davorg/perl-coveralls:latest<br>    name: Test coverage<br>    steps:<br>      - uses: actions/checkout@v3<br>      - name: Install modules<br>        run: cpanm -n --installdeps .<br>      - name: Coverage<br>        env:<br>          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}<br>        run: cover -test -report Coveralls</pre><p>And it’s producing nice graphs on Coveralls.io like the one above.</p><p>Let me know if you find it useful.</p><p><em>Originally published at </em><a href="https://perlhacks.com/2022/10/containers-for-coverage/"><em>Perl Hacks</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1db8dfc76c2d" width="1" height="1" alt="">       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">medium</span>

              <p class="text-right"><small>Published by Dave Cross
     on Tuesday 18 October 2022 17:02</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://perlhacks.com/2022/10/containers-for-coverage/">Containers for Coverage</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>I&#8217;ve been building Docker containers again. And I think you&#8217;ll find this one a little more useful than <a href="https://perlhacks.com/2022/10/building-a-perlanet-container/">the Perlanet one</a> I wrote about a couple of weeks ago.</p>
<p>Several years ago I got into <a href="https://www.travis-ci.com/">Travis CI</a> and set up lots of <a href="https://github.com/davorg?tab=repositories">my GitHub repos</a> so they automatically ran the tests each time I committed to the repo. Later on, I also worked out how to tie those test runs into <a href="https://coveralls.io/">Coveralls.io</a> so I got pretty graphs of how my test coverage was looking. I gave <a href="https://www.slideshare.net/davorg/github-travisci-and-perl">a talk about what I had done</a>.</p>
<p>But two things changed.</p>
<p>Firstly, Travis CI got too popular and, eventually, removed their free service. And, secondly, <a href="https://github.com/features/actions">GitHub Actions</a> was introduced. Over the last few years, I&#8217;ve set up many of my repos to use GitHub Actions for CI. But, basically because I&#8217;m lazy, I didn&#8217;t remove the Travis CI configuration from those repos.</p>
<p>But last week I decided the time was right to start work on that. And when I went to remove the .travis.yml I realised that something was missing from my GitHub Actions CI workflows &#8211; they were running the unit tests, but they weren&#8217;t reporting on test coverage. So it was time to fix that.</p>
<p>I needed to reimplement the logic that connected Travis CI to Coveralls.io in a GitHub workflow. That actually turned out to be pretty simple. There&#8217;s a CPAN module called <a href="https://metacpan.org/pod/Devel::Cover::Report::Coveralls">Devel::Cover::Report::Coveralls</a> which takes the output from Devel::Cover, converts it to the correct format and sends it to Coveralls.io. And, as a bonus, it has documentation showing how to implement that in a GitHub workflow.</p>
<p>So I hacked at my workflow definition file for one of my CPAN modules and within a few minutes I had it working.</p>
<p>Well, I say &#8220;a few minutes&#8221;, but it took over thirteen minutes to run. It turns out that Devel::Cover::Report::Coveralls is a pretty heavyweight module and needs to install a lot of other modules in order to do its work.</p>
<p>At this point, you can probably guess where this is going. And you&#8217;d be right.</p>
<p>I&#8217;ve created a Docker container that has Devel::Cover::Report::Coveralls already installed. And, obviously, it&#8217;s available for everyone to use from the Docker hub &#8211; <a href="https://hub.docker.com/repository/docker/davorg/perl-coveralls">davorg/perl-coveralls</a>.</p>
<p>A couple of small adjustments to my GitHub workflow and the coverage job is now running on my new container &#8211; and takes 29 seconds instead of 13 minutes. So that&#8217;s a win.</p>
<p>The relevant section of my <a href="https://github.com/davorg/calendar-simple/blob/master/.github/workflows/perltest.yml">workflow file</a> is here:</p><pre class="urvanov-syntax-highlighter-plain-tag">coverage:
    runs-on: ubuntu-latest
    container: davorg/perl-coveralls:latest
    name: Test coverage
    steps:
      - uses: actions/checkout@v3
      - name: Install modules
        run: cpanm -n --installdeps .
      - name: Coverage
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: cover -test -report Coveralls</pre><p>And it&#8217;s producing nice graphs on Coveralls.io like the one above.</p>
<p>Let me know if you find it useful.</p>
<p>The post <a rel="nofollow" href="https://perlhacks.com/2022/10/containers-for-coverage/">Containers for Coverage</a> appeared first on <a rel="nofollow" href="https://perlhacks.com">Perl Hacks</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">perl hacks</span>

              <p class="text-right"><small>Published by Dave Cross
     on Tuesday 18 October 2022 17:01</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://davorg.medium.com/not-that-pr-thanks-a1b15653a75?source=rss-3781feb8a4f4------2">Not that PR, thanks</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><figure><img alt="" src="https://cdn-images-1.medium.com/max/966/0*pLISkpVDhfg0_iwn.png" /></figure><p>It’s October. And that means that <a href="https://hacktoberfest.com/">Hacktoberfest</a> has started. If you can get four pull requests accepted on other people’s code repositories during October then you can win a t-shirt.</p><p>In many ways, I think it’s a great idea. It encourages people to get involved in open source software. But in other ways, it can be a bit of a pain in the arse. Some people go crazy for a free t-shirt and that means you’ll almost certainly get several pull requests that aren’t really of the quality you’d hope for.</p><p>I have a particular problem that probably isn’t very common. I’ve talked before about the “semi-static” sites I run on GitHub Pages. There’s some data in a GitHub Repo and every couple of hours the system wakes up and runs some code which generates a few HTML pages and commits those HTML pages into the repo’s “/docs” directory. And — hey presto! — there’s a new version of your web site.</p><p>A good example is <a href="https://perl.theplanetarium.org/">Planet Perl</a>. The data is a YAML file which mostly consists of a list of web feeds. Every couple of hours we run <a href="https://metacpan.org/dist/Perlanet/view/bin/perlanet">perlanet</a> to pull in those web feeds and build a new version of the web site containing the latest articles about Perl.</p><p>Can you see what the problem is?</p><p>The problem is that the most obvious file in the repo is the “index.html” which is the web site. So when people find that repo and want to make a small change to the web site they’ll change that “index.html” file. But that file is generated. Every few hours, any changes to that file are overwritten as a new version is created. You actually want to change “index.tt”. But that uses <a href="https://metacpan.org/dist/Template-Toolkit/view/lib/Template/Manual/Directives.pod">Template Toolkit syntax</a>, so it’s easy enough to see why people with no Perl knowledge might want to avoid editing that.</p><p>The <a href="https://github.com/davorg/planetperl/blob/master/README.md">README file</a> for the project explains which files you might want to change in order to make different types of changes. But people don’t read that. Or, if they do read it, they ignore the bits that they don’t like.</p><p>So I get pull requests that I have to reject because they change the wrong files.</p><p>Last year I got enough of these problematic pull requests that I decided to automate a solution. And it’s <a href="https://github.com/davorg/planetperl/blob/master/.github/workflows/reject-pr.yml">this pretty simple GitHub Workflow</a>. It runs whenever my repo receives a pull request and looks at the files that have been changed. If that list of files includes “docs/index.html” then the PR is automatically closed with a polite message explaining what they’ve done wrong.</p><p>This makes my life easier. It’s possible it might make your life easier too.</p><p><em>Originally published at </em><a href="https://perlhacks.com/2022/10/not-that-pr-thanks/"><em>Perl Hacks</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a1b15653a75" width="1" height="1" alt="">       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">medium</span>

              <p class="text-right"><small>Published by Dave Cross
     on Monday 10 October 2022 16:02</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://perlhacks.com/2022/10/not-that-pr-thanks/">Not that PR, thanks</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>It&#8217;s October. And that means that <a href="https://hacktoberfest.com/">Hacktoberfest</a> has started. If you can get four pull requests accepted on other people&#8217;s code repositories during October then you can win a t-shirt.</p>
<p>In many ways, I think it&#8217;s a great idea. It encourages people to get involved in open source software. But in other ways, it can be a bit of a pain in the arse. Some people go crazy for a free t-shirt and that means you&#8217;ll almost certainly get several pull requests that aren&#8217;t really of the quality you&#8217;d hope for.</p>
<p>I have a particular problem that probably isn&#8217;t very common. I&#8217;ve talked before about the &#8220;semi-static&#8221; sites I run on GitHub Pages. There&#8217;s some data in a GitHub Repo and every couple of hours the system wakes up and runs some code which generates a few HTML pages and commits those HTML pages into the repo&#8217;s &#8220;/docs&#8221; directory. And &#8211; hey presto! &#8211; there&#8217;s a new version of your web site.</p>
<p>A good example is <a href="https://perl.theplanetarium.org/">Planet Perl</a>. The data is a YAML file which mostly consists of a list of web feeds. Every couple of hours we run <a href="https://metacpan.org/dist/Perlanet/view/bin/perlanet">perlanet</a> to pull in those web feeds and build a new version of the web site containing the latest articles about Perl.</p>
<p>Can you see what the problem is?</p>
<p>The problem is that the most obvious file in the repo is the &#8220;index.html&#8221; which is the web site. So when people find that repo and want to make a small change to the web site they&#8217;ll change that &#8220;index.html&#8221; file. But that file is generated. Every few hours, any changes to that file are overwritten as a new version is created. You actually want to change &#8220;index.tt&#8221;. But that uses <a href="https://metacpan.org/dist/Template-Toolkit/view/lib/Template/Manual/Directives.pod">Template Toolkit syntax</a>, so it&#8217;s easy enough to see why people with no Perl knowledge might want to avoid editing that.</p>
<p>The <a href="https://github.com/davorg/planetperl/blob/master/README.md">README file</a> for the project explains which files you might want to change in order to make different types of changes. But people don&#8217;t read that. Or, if they do read it, they ignore the bits that they don&#8217;t like.</p>
<p>So I get pull requests that I have to reject because they change the wrong files.</p>
<p>Last year I got enough of these problematic pull requests that I decided to automate a solution. And it&#8217;s <a href="https://github.com/davorg/planetperl/blob/master/.github/workflows/reject-pr.yml">this pretty simple GitHub Workflow</a>. It runs whenever my repo receives a pull request and looks at the files that have been changed. If that list of files includes &#8220;docs/index.html&#8221; then the PR is automatically closed with a polite message explaining what they&#8217;ve done wrong.</p>
<p>This makes my life easier. It&#8217;s possible it might make your life easier too.</p>
<p>&nbsp;</p>
<p>The post <a rel="nofollow" href="https://perlhacks.com/2022/10/not-that-pr-thanks/">Not that PR, thanks</a> appeared first on <a rel="nofollow" href="https://perlhacks.com">Perl Hacks</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">perl hacks</span>

              <p class="text-right"><small>Published by Dave Cross
     on Monday 10 October 2022 16:01</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://davorg.medium.com/building-a-perlanet-container-86479f088e32?source=rss-3781feb8a4f4------2">Building a Perlanet Container</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Qurz8La174s8M_YL.png" /></figure><p>I’m a dinosaur who still believes that web feeds are a pretty neat idea. I wrote and maintain <a href="https://metacpan.org/pod/perlanet">perlanet</a> (a Perl program for aggregating web feeds into a new feed — and building a web site based on that new feed) and I use it to build <a href="https://theplanetarium.org/">a few sites on topics I’m interested in</a>.</p><p>Last year, I worked out a way to use GitHub Actions to <a href="https://dev.to/davorg/github-actions-for-semi-static-web-sites-597g">rebuild these sites automatically</a> every few hours — thereby enabling me to host the sites on GitHub Pages (I still think it’s a useful technique, but I sometimes worry slightly about the large number of commits those repos have — someone at GitHub must surely notice one day!)</p><p>Yesterday, I was doing some light maintenance on one of those sites when I realised that each rebuild of these sites was taking a significant time (by which I mean four or five minutes) and started wondering if there was a way to speed them up and use less of GitHub’s resources. The problem is that Perlanet is a pretty hefty module and each rebuild was installing that module (and, therefore, dozens of other modules) on a pristine Ubuntu container.</p><p>When you say it like that, the solution is obvious.</p><p>You don’t need to run your GitHub Actions on the standard containers that GitHub supplies. You can run them on any container that’s available from any public container hosting service. So the solution was to build a Perlanet container and run the jobs using that instead. So that’s how I spent an hour or so yesterday.</p><p>Here’s the Dockerfile I ended up with:</p><pre>FROM perl:latest<br><br>RUN apt-get update &amp;&amp; \<br>    apt-get -y upgrade &amp;&amp; \ <br>    apt-get install -y build-essential &amp;&amp; \<br>    apt-get install -y cpanminus libtidy-dev libxml++2.6-dev libhtml-tidy-perl &amp;&amp; \<br>    cpanm --notest Test::Exception &amp;&amp; \<br>    cpanm --notest Perlanet &amp;&amp; \<br>    cpanm --notest LWP::Protocol::https</pre><p>It’s (obviously) <a href="https://github.com/davorg/perl-perlanet-docker">available on GitHub</a> in case anyone wants to improve on my rather ropey knowledge of Docker.</p><p>I explicitly install Test::Exception because HTML::Tidy (one of Perlanet’s pre-requisites) needs it and I can’t work out why the standard installation procedure isn’t installing it. And while, LWP::Protocol::https is, strictly speaking, required by Perlanet, you wouldn’t get very far on the modern web if you only accessed web feeds that are available over HTTP.</p><p>A little bit of Docker Hub set-up and the container is <a href="https://hub.docker.com/r/davorg/perl-perlanet">available for everyone to use</a> (and rebuilt automatically whenever I commit to the repo).</p><p>It was then just a case of changing my GitHub Actions to use my container. Here’s an example of <a href="https://github.com/davorg/planetdavorg/commit/5b65ce407e89dd16e74ad4b417b104c43dca96ef">one of the commits</a> that did that.</p><p>I realise I’m pretty late to the party here, but I think this is a useful pattern. If you have a Perl library (or, indeed, any other software) that exists to provide a service to users then it’s a great idea to provide a containerised version of that software.</p><p>And I’m happy to report that my site rebuilds have gone from 4–5 minutes to about 45 seconds.</p><p><em>Originally published at </em><a href="https://perlhacks.com/2022/10/building-a-perlanet-container/"><em>Perl Hacks</em></a><em>.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=86479f088e32" width="1" height="1" alt="">       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">medium</span>

              <p class="text-right"><small>Published by Dave Cross
     on Thursday 06 October 2022 09:44</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://davorg.medium.com/2021-in-gigs-7ab20766b20f?source=rss-3781feb8a4f4------2">2021 in Gigs</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*a_nd3y_-Pb0z6dI0.jpg" /></figure><p>A little later than usual, here’s my review of the gigs I saw last year.</p><p>In 2020, I saw four gigs. In 2021, I almost doubled that to seven. Obviously, we spent a lot of the year with most music venues closed, so those few gigs I saw were all in the second half of the year. Usually, I’d list my top ten gigs. This year (as last year) I’ll be listing them all. So here they are in chronological order.</p><ul><li><a href="https://www.songkick.com/concerts/39755096-mike-oldfield-at-royal-festival-hall-southbank-centre"><strong>Tubular Bells at the Royal Festival Hall</strong></a><br>This was a strange show for several reasons. Firstly, it was advertised as commemorating the fiftieth anniversary of <em>Tubular Bells</em>. But the album was released in 1973, so it was two years early (apparently it was the fiftieth anniversary of when Mike Oldfield started writing the piece). Secondly, Mike Oldfield wasn’t performing — but you needed to examine the publicity very carefully to work that out. And thirdly, there was a troupe of acrobats that were pointlessly leaping around the stage while the musicians played. All in all, I thought this was slightly disappointing.</li><li><a href="https://www.songkick.com/concerts/38968289-heaven-17-at-roundhouse"><strong>Heaven 17 at the Roundhouse</strong></a><br>Many of these shows were postponed from 2020. This was originally intended to celebrate the fortieth anniversary of the Human League album, <em>Travelogue</em>, but it ended but being the forty-first anniversary. But none of that mattered. This was Heaven 17 playing all of the first two Human League albums and it was absolutely wonderful. Apparently, they had invited Phil Oakey to take part, but he wasn’t interested. That’s Heaven 17 in the photo above.</li><li><a href="https://www.songkick.com/concerts/39828850-lump-at-scala"><strong>LUMP at the Scala</strong></a><br>LUMP is Laura Marling playing with Tunng’s Mike Lindsay. I kinda assumed that their first album was going to be a one-off, but they produced a second album in 2020. This was the first gig I’d been to in a cramped venue like the Scala for a couple of years and it all got a bit too much for me. I really didn’t enjoy the atmosphere and left during the third or fourth song. I still love the album though and I hope to build up my tolerance for gig crowds over the coming months.</li><li><a href="https://www.songkick.com/concerts/39773071-staves-at-o2-shepherds-bush-empire"><strong>The Staves at Shepherd’s Bush Empire</strong></a><br>Actually, this was only two-thirds of the Staves. One of the sisters has had a baby recently and has decided to sit out tours for a couple of years. But the two remaining sisters still put on a great show.</li><li><a href="https://www.songkick.com/concerts/39786793-laura-marling-at-roundhouse"><strong>Laura Marling at the Roundhouse</strong></a><br>Given how few gigs I saw last year, it’s surprising how repetitive they were. Here’s Laura Marling again (and the Roundhouse again!) Although she has yet to match the heights of the <em>Short Movie</em> tour, Laura Marling is always worth seeing and this show was no exception.</li><li><a href="https://www.songkick.com/concerts/39662320-heaven-17-at-o2-shepherds-bush-empire"><strong>Heaven 17 at the Shepherd’s Bush</strong></a><br>More repetition. I think the two Heaven 17 gigs were originally supposed to be several months apart, but the vagaries of the Covid scheduling changes led to them being just two months apart. This one celebrated the fortieth (actually forty-first) anniversary of Heaven 17 starting and was a glorious journey through their back catalogue. Oh, and the support was Pete Wylie, so I can finally say I’ve seen all three members of the <a href="https://en.wikipedia.org/wiki/Crucial_Three">Crucial Three</a> live.</li><li><a href="https://www.songkick.com/concerts/39709208-orchestral-manoeuvres-in-the-dark-at-eventim-apollo"><strong>Orchestral Manoeuvres in the Dark at Hammersmith Apollo</strong></a><br>OMD are just one of those bands that I see live whenever I can. I’ve now been seeing them for over forty years (since they supported Gary Numan in 1980). They have such a massive back catalogue that they can just play hit after hit for two hours. But this show was a bit different as they started by playing all of their 1981 album, <em>Architecture and Morality</em>. They were as good as I’ve ever seen them.</li></ul><p>And that was 2021. What will happen in 2022? Well, I have tickets for a dozen or so shows but who knows how many of them I’ll actually see? I’ve already had emails postponing the Wolf Alice and Peter Hook shows I was going to see this month. I guess I’ll just have to wait and see how the rest of the year pans out.</p><p><em>Originally published at </em><a href="https://blog.dave.org.uk/2022/01/2021-in-gigs.html"><em>https://blog.dave.org.uk</em></a><em> on January 9, 2022.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7ab20766b20f" width="1" height="1" alt="">       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">medium</span>

              <p class="text-right"><small>Published by Dave Cross
     on Sunday 09 January 2022 20:06</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://blog.dave.org.uk/2022/05/should-children-be-polite-while-using-smart-speakers.html">Should Children be Polite While Using Smart Speakers?</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>‘Okay Google. Where is Antarctica?”</p>
<p>Children can now get answers to all their questions using smart speakers and digital voice assistants.</p>
<p>A few years ago, children would run to their parents or grandparents to answer their questions. But with the ascendence of voice assistants to the mainstream in recent years, many children rely more on technology than humans.</p>
<p>Is this a good idea?</p>
<p>How does it impact the children?</p>
<p>When children interact with people, it helps them be more thoughtful, creative, and imaginative.</p>
<p>When they use artificial intelligence instead, several issues come into the foreground. These include access to age-inappropriate content and increasing the possibility of being rude or unpleasant, affecting how they treat others.</p>
<p>As mentioned, technology has both pros and cons. There are benefits to children using these devices, including improving diction, communication, social skills, and gaining information without bothering their parents.</p>
<p>Many families find that smart speakers like Amazon Echo and Google Home are useful. They use them for several functions, ranging from answering questions to setting the thermostat. Research shows that up to nine out of ten children between the ages of four and eleven in the US are regularly using smart speakers — often without parental guidance and control. So, what is the best approach for a parent to take?</p>
<p>Children up to seven years old can find it challenging to differentiate between humans and devices, and this can lead to one of the biggest dangers. If the device fulfils their requests through rude behaviour, children may behave similarly to other humans.</p>
<h2>Do Parents Think Smart Devices Should Encourage Polite Conversations?</h2>
<p>Most parents consider it essential that smart devices should encourage polite conversations as a part of nurturing good habits in children. The Campaign for a Commercial-Free Childhood or CCFA is a US coalition of concerned parents, healthcare professionals, and educators. Recently, CCFA protested against Amazon Echo Dot Kids Edition, stating that it may affect children’s wellbeing. Because of this, they requested parents avoid buying Amazon Echo.</p>
<p>However, in reality, these smart devices have improved a lot and focus on encouraging polite conversations with children. It is all about how parents use and present these devices to their children, as these factors can influence them a lot.</p>
<p>But in simple terms, parents wish these devices to encourage politeness in their children. At the same time, they want their kids to understand the difference between artificial intelligence and humans while using these technological innovations.</p>
<h2>Do Parents Think Their Children are Less Polite While Using Smart Speakers?</h2>
<p>Many parents have seen their children behave rudely to smart speakers. Several parents have expressed their concerns through social media, blog posts and forums like Mumsnet. They fear these behaviours can impact their kids when they grow up.</p>
<p>A report published in Child Wise reached the conclusion that children who behave rudely to smart devices might be aggressive while they grow up, especially while dealing with other humans. It is, therefore, preferable if children use polite words while interacting with both humans and smart devices.</p>
<h2>What Approaches Have Been Taken By Tech Companies to Address the Problem?</h2>
<p>With interventions and rising concerns addressed by parents and health professionals, some tech companies have brought changes to virtual assistants and smart speakers.</p>
<p>The parental control features available in Alexa focus on training kids to be more polite. Amazon brands it as Magic Word, where the focus is on bringing positive enforcement. However, there is no penalty if children don’t speak politely. Available on Amazon Echo, this tool has added features like setting bedtimes, switching off devices, and blocking songs with explicit lyrics.</p>
<p>When it comes to Google Home, it has brought in a new feature called Pretty Please. Here, Google will perform an action only when children use, please. For instance, “Okay, Google. Please set the timer for 15 minutes.”</p>
<p>You can enable this feature through the Google Family Link, where you can find the settings for Home and Assistant. You can set these new standards for devices of your preference. Also, once you use it and figure things out, there will be no more issues in setting it up again.</p>
<p>These tools and their approaches are highly beneficial for kids and parents. As of now, these devices only offer basic features and limited replies. But with time, there could be technological changes that encourage children to have much more efficient and polite interactions.</p>
<h2>George and the Smart Home</h2>
<p>It was thinking about issues like this which led me to write my first children’s book — George and the Smart Home. In the book, George is a young boy who has problems getting the smart speakers in his house to do what he wants until he learns to be polite to them.</p>
<p>It is available now, as a paperback and a Kindle book, from Amazon.</p>
<p>Buy it from: <a href="https://amazon.com.au/dp/B09VPLGMBN">AU</a> / <a href="https://amazon.com.br/dp/B09VPLGMBN">BR</a> / <a href="https://amazon.ca/dp/B09VPLGMBN">CA</a> / <a href="https://amazon.de/dp/B09VPLGMBN">DE</a> / <a href="https://amazon.es/dp/B09VPLGMBN">ES</a> / <a href="https://amazon.fr/dp/B09VPLGMBN">FR</a> / <a href="https://amazon.in/dp/B09VPLGMBN">IN</a> / <a href="https://amazon.it/dp/B09VPLGMBN">IT</a> / <a href="https://amazon.co.jp/dp/B09VPLGMBN">JP</a> / <a href="https://amazon.com.mx/dp/B09VPLGMBN">MX</a> / <a href="https://amazon.nl/dp/B09VPLGMBN">NL</a> / <a href="https://amazon.co.uk/dp/B09VPLGMBN">UK</a> / <a href="https://amazon.com/dp/B09VPLGMBN">US</a></p>
<p>The post <a rel="nofollow" href="https://blog.dave.org.uk/2022/05/should-children-be-polite-while-using-smart-speakers.html">Should Children be Polite While Using Smart Speakers?</a> appeared first on <a rel="nofollow" href="https://blog.dave.org.uk">Davblog</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">davblog</span>

              <p class="text-right"><small>Published by Dave Cross
     on Monday 02 May 2022 13:09</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://blog.dave.org.uk/2022/01/2021-in-gigs.html">2021 in Gigs</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p>A little later than usual, here&#8217;s my review of the gigs I saw last year.</p>
<p>In 2020, I saw four gigs. In 2021, I almost doubled that to seven. Obviously, we spent a lot of the year with most music venues closed, so those few gigs I saw were all in the second half of the year. Usually, I&#8217;d list my top ten gigs. This year (as last year) I&#8217;ll be listing them all. So here they are in chronological order.</p>
<ul>
<li><strong><a href="https://www.songkick.com/concerts/39755096-mike-oldfield-at-royal-festival-hall-southbank-centre">Tubular Bells at the Royal Festival Hall</a></strong><br />
This was a strange show for several reasons. Firstly, it was advertised as commemorating the fiftieth anniversary of <em>Tubular Bells</em>. But the album was released in 1973, so it was two years early (apparently it was the fiftieth anniversary of when Mike Oldfield started writing the piece). Secondly, Mike Oldfield wasn&#8217;t performing &#8211; but you needed to examine the publicity very carefully to work that out. And thirdly, there was a troupe of acrobats that were pointlessly leaping around the stage while the musicians played. All in all, I thought this was slightly disappointing.</li>
<li><strong><a href="https://www.songkick.com/concerts/38968289-heaven-17-at-roundhouse">Heaven 17 at the Roundhouse</a></strong><br />
Many of these shows were postponed from 2020. This was originally intended to celebrate the fortieth anniversary of the Human League album, <em>Travelogue</em>, but it ended but being the forty-first anniversary. But none of that mattered. This was Heaven 17 playing all of the first two Human League albums and it was absolutely wonderful. Apparently, they had invited Phil Oakey to take part, but he wasn&#8217;t interested. That&#8217;s Heaven 17 in the photo above.</li>
<li><strong><a href="https://www.songkick.com/concerts/39828850-lump-at-scala">LUMP at the Scala</a></strong><br />
LUMP is Laura Marling playing with Tunng&#8217;s Mike Lindsay. I kinda assumed that their first album was going to be a one-off, but they produced a second album in 2020. This was the first gig I&#8217;d been to in a cramped venue like the Scala for a couple of years and it all got a bit too much for me. I really didn&#8217;t enjoy the atmosphere and left during the third or fourth song. I still love the album though and I hope to build up my tolerance for gig crowds over the coming months.</li>
<li><strong><a href="https://www.songkick.com/concerts/39773071-staves-at-o2-shepherds-bush-empire">The Staves at Shepherd&#8217;s Bush Empire</a></strong><br />
Actually, this was only two-thirds of the Staves. One of the sisters has has a baby recently and has decided to sit out tours for a couple of years. But the two remaining sisters still put on a great show.</li>
<li><strong><a href="https://www.songkick.com/concerts/39786793-laura-marling-at-roundhouse">Laura Marling at the Roundhouse</a></strong><br />
Given how few gigs I saw last year, it&#8217;s surprising how repetitive they were. Here&#8217;s Laura Marling again (and the Roundhouse again!) Although she has yet to match the heights of the <em>Short Movie</em> tour, Laura Marling is always worth seeing and this show was no exception.</li>
<li><strong><a href="https://www.songkick.com/concerts/39662320-heaven-17-at-o2-shepherds-bush-empire">Heaven 17 at the Shepherd&#8217;s Bush</a></strong><strong><a href="https://www.songkick.com/concerts/39662320-heaven-17-at-o2-shepherds-bush-empire"> Empire</a></strong><br />
More repetition. I think the two Heaven 17 gigs were originally supposed to be several months apart, but the vagaries of the Covid scheduling changes led to them being just two months apart. This one celebrated the fortieth (actually forty-first) anniversary of Heaven 17 starting and was a glorious journey through their back catalogue. Oh, and the support was Pete Wylie, so I can finally say I&#8217;ve seen all three members of the <a href="https://en.wikipedia.org/wiki/Crucial_Three">Crucial Three</a> live.</li>
<li><strong><a href="https://www.songkick.com/concerts/39709208-orchestral-manoeuvres-in-the-dark-at-eventim-apollo">Orchestral Manoeuvres in the Dark at Hammersmith Apollo</a></strong><br />
OMD are just one of those bands that I see live whenever I can. I&#8217;ve now been seeing them for over forty years (since they supported Gary Numan in 1980). They have such a massive back catalogue that they can just play hit after hit for two hours. But this show was a bit different as they started by playing all of their 1981 album, <em>Architecture and Morality</em>. They were as good as I&#8217;ve ever seen them.</li>
</ul>
<p>And that was 2021. What will happen in 2022? Well, I have tickets for a dozen or shows but who knows how many of them I&#8217;ll actually see? I&#8217;ve already had emails postponing the Wolf Alice and Peter Hook shows I was going to see this month. I guess I&#8217;ll just have to wait and see how the rest of the year pans out.</p>
<p>The post <a rel="nofollow" href="https://blog.dave.org.uk/2022/01/2021-in-gigs.html">2021 in Gigs</a> appeared first on <a rel="nofollow" href="https://blog.dave.org.uk">Davblog</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">davblog</span>

              <p class="text-right"><small>Published by Dave Cross
     on Sunday 09 January 2022 20:06</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->
          <div class="panel panel-default">
            <div class="panel-heading">
              <h2><a href="https://blog.dave.org.uk/2021/09/the-return-of-rtd.html">The Return of RTD</a></h2>
            </div><!-- panel-heading -->
            <div class="panel-body"><p><em>Doctor Who</em> has a new showrunner. But he’s actually an old showrunner. Is that a good idea?</p>
<p>Since <a href="https://www.bbc.co.uk/news/entertainment-arts-58682472">the news broke</a> yesterday, <em>Doctor Who</em> fan forums have been discussing nothing but the fact that Russell T Davies is returning as showrunner after Chris Chibnall’s regeneration special is broadcast next year. Most fans seem to be very excited by this prospect; I’m not so sure.</p>
<p>Before I start, I should point out that I’ve been a big fan of Russell T Davies since long before he brought <em>Doctor Who</em> back to our screens in 2005. I’ll always be grateful for the work he did to bring the show back and I believe that he’s responsible for some great moments in <em>Doctor Who</em> history.</p>
<p>But I’m not sure I want to see him back as the showrunner. Let me explain why I’m so out of step with most of the show’s fans.</p>
<p>Firstly, although I’m grateful to him for bringing the show back, he’s not my favourite showrunner. Obviously, any <em>Doctor Who</em> is better than no <em>Doctor Who</em> but there was a lot of stuff in Davies’ first run that I didn’t like. For example, He was the person who first introduced us to companions’ families, which brought a slight soap opera feel to some of the episodes. Also, I thought that he often wrote himself into a bit of a corner. This was most apparent in the end of season two-parters. There were many occasions when the first part set up a fantastic premise only to be let down by a finale that just couldn’t live up to the promise. <em>The Stolen Earth</em> was great; <em>Journey’s End</em> was terrible. Then there’s <em>The End of Time</em>. Again, it started off well but had verged well into the ridiculous by the end of the first part. And don’t get me started on the self-indulgent, mawkish nonsense that made up the last twenty minutes of that story — leading to the Tenth Doctor’s regeneration.</p>
<p>I admit, however, that my opinions on Davies’ writing are purely personal. And, because of the massive rise in popularity of the show during his tenure, many viewers see his approach as the gold standard for how the show should work. My other points are, I hope, less opinion-based.</p>
<p>Secondly, <em>Doctor Who</em> is a show that should always be moving forward. In the classic era of the show, previous Doctors and companions would reappear very rarely. When someone left the show, you knew the chances of seeing them again were very slim. When an executive producer left (we didn’t call them showrunners back then) you knew that the show would change in new and experimental ways. Sometimes the changes didn’t work; most of the time they did. Change is fundamental to the show. It’s how the show has kept going for (most of) sixty years.</p>
<p>The newer sections of the audience don’t seem to realise that. I constantly hear fans wanting things to go back to how things were. As soon as Rose was written out at the end of series two, there were calls for her to come back. And while series four has some pretty good stuff in it, I think that bringing Rose back was pandering to the fanbase in an unhealthy way. We now have a situation where fans expect every character who has been written out of the show to be brought back at their whim. There aren’t very many weeks that pass without me seeing someone in a Facebook group suggesting some convoluted way that David Tennant could be brought back to be the Doctor again.</p>
<p>The show must always move forward. It must always change. I believe that RTD knows that, so I hope that his second era in charge will be sufficiently different to his first. But I worry that fans will start asking for Tennant back as the Doctor with Billie Piper by his side. For some fans, that seems to be the only version of the show they will be happy with.</p>
<p>Finally, I worry about what RTD’s reappointment means for the future of the show. When Chibnall’s departure was announced, all of the news stories claimed that he and Whittaker had a “three and out agreement” between themselves and that he only ever planned to do three years running the show. That’s rather at odds with the talk of him having a five-year plan for the show when he was appointed to the role. I realise that he will have done five years in the post by the time he goes, but he will have made three seasons and a handful of specials — so I’m not sure that counts.</p>
<p>No, I think it’s clear that Chibnall has been hounded out of the role by that toxic sector of the fanbase that refuses to give his work on the show a decent chance. And, given that Moffat also put up with a lot of abuse from certain fans, I begin to wonder how easy it is to find someone to take over the job. Chibnall’s departure was announced at the end of July and the BBC would certainly have known about it for some time before that. But they have failed to find someone new and exciting to take over the job and I wonder if it has become a bit of a poison chalice. People want to do the job because, hey, it’s running Doctor Who! But, on the other hand, if you don’t please the fanbase (and no-one can please all of the fanbase) then you’ll be vilified online and hounded off social media. Add to that the fact that both Davies and Moffat cited insane working schedules as part of their reason for leaving and, suddenly, the job doesn’t look quite as tempting.</p>
<p>I have no inside information here at all, but I wonder if the reappointment of RTD was an act of desperation on the part of the BBC. We know that Chibnall is steering the show up to and including a BBC centenary special that will be broadcast in 2022. But the show’s 60th anniversary is the year after that and without a showrunner, you can’t cast a new Doctor and without a new Doctor in place pretty soon, the 60th-anniversary celebrations would seem to be in danger.</p>
<p>The news of the reappointment has all been very celebratory, of course, but I wonder if that’s actually the case. I wonder if the BBC’s approach to RTD was more like this:</p>
<p>“So, that show you resurrected back in 2005. Well, we can’t find anyone to take over as showrunner, and unless we get things moving pretty quickly we’re not going to have a 60th anniversary worth speaking off. Seriously, we’re thinking of just cancelling it… unless you can suggest something that we could do…”</p>
<p>This, of course, leaves RTD thinking that the only way to save his baby is to step in himself. Maybe he’s stepped in as a stop-gap until the BBC finds someone else to take over. The announcement says he’s signed on for the 60th special and following series. But that’s a bit vague (because the English language doesn’t have a plural for “series”!) so who knows how long he’ll hang around for. Time will tell, I guess.</p>
<p>But, if you’re one of those fans who think it’s big or clever to be unrelentingly negative about the showrunner on social media, please stop and consider whether you’re part of a problem that could end up with no-one wanting the job and the show being cancelled.</p>
<p>All-in-all, I wish that the BBC hadn’t done this. I would have far preferred to see the show moving forward. But if, as I suspect, the alternative was no new Doctor Who for the foreseeable future, then obviously this is a good plan. I’m keen to see what Davies has in store.</p>
<p>But first I’m really excited to see what Chibnall has in store for his final series and the subsequent specials. If series 13 improves on series 12 to the extent that series 12 improved on series 11, then it’s going to be great.</p>
<p>The post <a rel="nofollow" href="https://blog.dave.org.uk/2021/09/the-return-of-rtd.html">The Return of RTD</a> appeared first on <a rel="nofollow" href="https://blog.dave.org.uk">Davblog</a>.</p>
       </div><!-- panel-body -->
            <div class="panel-footer">
<span class="label label-default" style="float: left">davblog</span>

              <p class="text-right"><small>Published by Dave Cross
     on Saturday 25 September 2021 12:59</small></p>
            </div><!-- panel-footer -->
          </div><!-- panel -->

        </div><!-- col-md-8 -->
        <div id="col-md-1">
<script type="text/javascript"><!--
google_ad_client = "pub-3663319204757145";
/* planet */
google_ad_slot = "8413744374";
google_ad_width = 120;
google_ad_height = 600;
//-->
</script>
<script type="text/javascript"
src="http://pagead2.googlesyndication.com/pagead/show_ads.js">
</script>
        </div><!-- col-md-1 -->
      </div><!-- row -->
      <div class="row">
        <div class="col-md-10">
      <address><a href="mailto:dave@theplanetarium.org">Dave Cross</a> / Friday 21 April 2023 01:33</address>
        </div><!-- col-md-10 -->
      </div><!-- row -->
    </div><!-- container -->
    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>

  </body>
</html>
